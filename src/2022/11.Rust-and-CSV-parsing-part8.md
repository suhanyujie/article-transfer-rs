>* Rust and CSV parsing 译文（用 Rust 实现 csv 解析-part8）
>* 原文链接：https://blog.burntsushi.net/csv/
>* 原文作者：[BurntSushi](https://github.com/BurntSushi)
>* 译文来自：https://github.com/suhanyujie/article-transfer-rs/
>* 译者：[suhanyujie](https://ishenghuo.cnblogs.com/)
>* ps：水平有限，如有不当之处，欢迎指正
>* 标签：Rust，csv

## Performance
In this section, we’ll go over how to squeeze the most juice out of our CSV reader. As it happens, most of the APIs we’ve seen so far were designed with high level convenience in mind, and that often comes with some costs. For the most part, those costs revolve around unnecessary allocations. Therefore, most of the section will show how to do CSV parsing with as little allocation as possible.
> 在本节中，我们将讨论如何最大限度地利用 CSV reader。实际上，到目前为止，我们所看到的大多数 api 在设计时都考虑到了高度的易用性，而这通常需要付出一些代价。大多数情况下，这些成本会带来一些不必要的分配。因此，本节的大部分内容将展示如何使用尽可能少的分配来进行 CSV 解析。

There are two critical preliminaries we must cover.
> 有两个必须涉及且比较关键的先决条件。

Firstly, when you care about performance, you should compile your code with `cargo build --release` instead of `cargo build`. The `--release` flag instructs the compiler to spend more time optimizing your code. When compiling with the `--release` flag, you’ll find your compiled program at `target/release/csvtutor` instead of `target/debug/csvtutor`. Throughout this tutorial, we’ve used `cargo build` because our dataset was small and we weren’t focused on speed. The downside of cargo build `--release` is that it will take longer than cargo build.
> 首先，当你关心性能时，你应该使用 `cargo build --release` 而不是 `cargo build` 来编译代码。`——release` 标志表示编译器花一些时间优化你的代码。当使用 `——release` 标志编译时，你会发现编译后的程序在 `target/release/csvtutor` 而不是 `target/debug/csvtutor` 。在本教程中，我们使用 crate 构建，因为我们的数据集很小，我们不关注速度。crate 构建时使用 `--release` 的缺点是，它将花费更长的构建时间。

Secondly, the dataset we’ve used throughout this tutorial only has 100 records. We’d have to try really hard to cause our program to run slowly on 100 records, even when we compile without the --release flag. Therefore, in order to actually witness a performance difference, we need a bigger dataset. To get such a dataset, we’ll use the original source of uspop.csv. **Warning: the download is 41MB compressed and decompresses to 145MB.**
> 其次，我们在本教程中使用的数据集只有 100 条记录。我们必须非常努力地使程序在 100 条记录上运行缓慢，即使在没有 `--release` 标志的情况下进行编译。因此，为了真正见证性能差异，我们需要更大的数据集。为了获得这样的数据集，我们将使用 uspop.csv 的原始源。**警告:下载 41MB 解压后将有 145MB**

```$ curl -LO https://burntsushi.net/stuff/worldcitiespop.csv.gz
$ gunzip worldcitiespop.csv.gz
$ wc worldcitiespop.csv
  3173959   5681543 151492068 worldcitiespop.csv
$ md5sum worldcitiespop.csv
6198bd180b6d6586626ecbf044c1cca5  worldcitiespop.csvshell
```

Finally, it’s worth pointing out that this section is not attempting to present a rigorous set of benchmarks. We will stay away from rigorous analysis and instead rely a bit more on wall clock times and intuition.
> 最后，需要指出的是，本节并不是要提供一套严格的基准。我们将不会很严格地分析，而是更多地依靠挂钟时间和直觉。

## Amortizing allocations

In order to measure performance, we must be careful about what it is we’re measuring. We must also be careful to not change the thing we’re measuring as we make improvements to the code. For this reason, we will focus on measuring how long it takes to count the number of records corresponding to city population counts in Massachusetts. This represents a very small amount of work that requires us to visit every record, and therefore represents a decent way to measure how long it takes to do CSV parsing.
> 为了衡量性能，我们必须小心我们所衡量的是什么。在改进代码时，我们还必须注意不要改变我们正在测量的东西。出于这个原因，我们将重点测量需要多长时间来统计与马萨诸塞州城市人口统计相对应的记录数量。这表示需要我们访问每条记录的工作量非常小，因此这是衡量执行 CSV 解析所需时间的一种不错的方法。

Before diving into our first optimization, let’s start with a baseline by adapting a previous example to count the number of records in `worldcitiespop.csv`:
> 在开始我们的第一个优化之前，让我们从一个基础开始，通过调整之前的示例来计算`worldcitiespop.csv` 中的记录数量：

```rust
extern crate csv;

use std::error::Error;
use std::io;
use std::process;

fn run() -> Result<u64, Box<Error>> {
    let mut rdr = csv::Reader::from_reader(io::stdin());

    let mut count = 0;
    for result in rdr.records() {
        let record = result?;
        if &record[0] == "us" && &record[3] == "MA" {
            count += 1;
        }
    }
    Ok(count)
}

fn main() {
    match run() {
        Ok(count) => {
            println!("{}", count);
        }
        Err(err) => {
            println!("{}", err);
            process::exit(1);
        }
    }
}
```

Now let’s compile and run it and see what kind of timing we get. Don’t forget to compile with the --release flag. (For grins, try compiling without the `--release` flag and see how long it takes to run the program!)
> 现在，我们编译并运行它，看看我们使用了多少时间。不要忘记使用 `--release` 标志进行编译。（嘿嘿，尝试编译不带 `--release` 标志，看看运行程序需要多长时间！）

```shell
$ cargo build --release
$ time ./target/release/csvtutor < worldcitiespop.csv
2176

real    0m0.645s
user    0m0.627s
sys     0m0.017s
```

All right, so what’s the first thing we can do to make this faster? This section promised to speed things up by amortizing allocation, but we can do something even simpler first: iterate over ByteRecords instead of StringRecords. If you recall from a previous section, a StringRecord is guaranteed to be valid UTF-8, and therefore must validate that its contents is actually UTF-8. (If validation fails, then the CSV reader will return an error.) If we remove that validation from our program, then we can realize a nice speed boost as shown in the next example:
> 好了，我们要怎么做才能加快速度呢？本节承诺通过摊销分配来加快速度，但我们可以先做一些更简单的事情：迭代 `ByteRecords` 而非 `StringRecords`。如果您还记得上一节，`StringRecord` 保证是有效的 UTF-8，因此必须校验它的内容是否是 UTF-8。（如果验证失败，那么CSV读取器将返回一个错误。）如果我们从我们的程序中移除校验代码，那么我们可以实现一个很好的性能提升，如下面的例子所示:

```rust
fn run() -> Result<u64, Box<Error>> {
    let mut rdr = csv::Reader::from_reader(io::stdin());

    let mut count = 0;
    for result in rdr.byte_records() {
        let record = result?;
        if &record[0] == b"us" && &record[3] == b"MA" {
            count += 1;
        }
    }
    Ok(count)
}
```

编译并运行：

```shell
$ cargo build --release
$ time ./target/release/csvtutor < worldcitiespop.csv
2176

real    0m0.429s
user    0m0.403s
sys     0m0.023s
```

Our program is now approximately 30% faster, all because we removed UTF-8 validation. But was it actually okay to remove UTF-8 validation? What have we lost? In this case, it is perfectly acceptable to drop UTF-8 validation and use ByteRecord instead because all we’re doing with the data in the record is comparing two of its fields to raw bytes:
> 我们的程序现在大约快了 30%，这都是因为我们删除了 UTF-8 验证。但是，删除UTF-8 验证真的可以吗？我们失去了什么？在这种情况下，删除 UTF-8 校验并使用 `ByteRecord` 是完全可以接受的，因为我们对记录中的数据所做的是将其两个字段与原始字节进行比较：

```rust
if &record[0] == b"us" && &record[3] == b"MA" {
    count += 1;
}
```

In particular, it doesn’t matter whether record is valid UTF-8 or not, since we’re checking for equality on the raw bytes themselves.
> 特别是，记录是否是有效的 UTF-8 并不重要，因为我们正在检查它和原始字节是否相等。

UTF-8 validation via StringRecord is useful because it provides access to fields as &str types, where as ByteRecord provides fields as &[u8] types. &str is the type of a borrowed string in Rust, which provides convenient access to string APIs like substring search. Strings are also frequently used in other areas, so they tend to be a useful thing to have. Therefore, sticking with StringRecord is a good default, but if you need the extra speed and can deal with arbitrary bytes, then switching to ByteRecord might be a good idea.
> 通过 `StringRecord` 进行 UTF-8 校验很有用，因为它提供了对 `&str` 类型字段的访问，而 `ByteRecord` 提供了 `&[u8]` 类型字段。`&str` 是 Rust 中借用的字符串类型，它提供了对字符串 api 的便捷访问，比如子字符串搜索。 String 也经常用于其他领域，因此它们往往是一个有用的东西。因此，坚持使用 `StringRecord` 是一个很好的默认值，但是如果您需要额外的速度并且可以处理任意字节，那么切换到 `ByteRecord` 可能是一个好主意。

Moving on, let’s try to get another speed boost by amortizing allocation. Amortizing allocation is the technique that creates an allocation once (or very rarely), and then attempts to reuse it instead of creating additional allocations. In the case of the previous examples, we used iterators created by the records and byte_records methods on a CSV reader. These iterators allocate a new record for every item that it yields, which in turn corresponds to a new allocation. It does this because iterators cannot yield items that borrow from the iterator itself, and because creating new allocations tends to be a lot more convenient.
> 接下来，让我们通过摊销分配（amortizing allocation）来提高速度。摊销分配是一种技术，它只创建一次(或很少创建)分配，然后尝试重用它，而不是创建额外的分配。在前面的例子中，我们使用了由 CSV reader 上的 records 和 byte_records 方法创建的迭代器。这些迭代器为它产生的每一项分配一条新记录空间，对应于一个新的分配。之所以这样做，是因为迭代器不能产生借用迭代器本身的项，而且创建新的分配往往更方便。

If we’re willing to forgo use of iterators, then we can amortize allocations by creating a single ByteRecord and asking the CSV reader to read into it. We do this by using the [`Reader::read_byte_record`](https://docs.rs/csv/1.0.0/csv/struct.Reader.html#method.read_byte_record) method.
> 如果我们愿意放弃使用迭代器，那么可以通过创建单个 ByteRecord 并要求 CSV  reader 读入它来分摊分配。我们通过使用 [`Reader::read_byte_record`](https://docs.rs/csv/1.0.0/csv/struct.Reader.html#method.read_byte_record) 方法来实现这一点。

```rust
fn run() -> Result<u64, Box<Error>> {
    let mut rdr = csv::Reader::from_reader(io::stdin());
    let mut record = csv::ByteRecord::new();

    let mut count = 0;
    while rdr.read_byte_record(&mut record)? {
        if &record[0] == b"us" && &record[3] == b"MA" {
            count += 1;
        }
    }
    Ok(count)
}
```

编译并运行：

```shell
$ cargo build --release
$ time ./target/release/csvtutor < worldcitiespop.csv
2176

real    0m0.308s
user    0m0.283s
sys     0m0.023s
```

Woohoo! This represents another 30% boost over the previous example, which is a 50% boost over the first example.
> 哦吼！这比前一个例子又提升了 30%，比第一个例子提升了 50%。

Let’s dissect this code by taking a look at the type signature of the read_byte_record method:
> 让我们通过查看 read_byte_record 方法签名来分析这段代码：

```rust
fn read_byte_record(&mut self, record: &mut ByteRecord) -> csv::Result<bool>;
```

This method takes as input a CSV reader (the self parameter) and a mutable borrow of a ByteRecord, and returns a csv::Result<bool>. (The csv::Result<bool> is equivalent to Result<bool, csv::Error>.) The return value is true if and only if a record was read. When it’s false, that means the reader has exhausted its input. This method works by copying the contents of the next record into the provided ByteRecord. Since the same ByteRecord is used to read every record, it will already have space allocated for data. When read_byte_record runs, it will overwrite the contents that were there with the new record, which means that it can reuse the space that was allocated. Thus, we have amortized allocation.
> 该方法接受一个 CSV reader(self 参数)和一个可变的 ByteRecord 借用作为输入，并返回 `csv::Result<bool>`。（`csv::Result<bool>` 等价于 `Result<bool, csv::Error>` 当且仅当读取记录，返回值为 true 时）。当它为 false 时，这意味着 reader 已经耗尽了输入。该方法通过将下一条记录的内容复制到提供的 ByteRecord 中来工作。由于使用同一个 ByteRecord 读取每条记录，因此它已经为数据分配了空间。当调用 read_byte_record 时，它将用新记录覆盖原来存在的内容，这意味着它可以复用已分配的空间。因此，我们平摊了分配。

An exercise you might consider doing is to use a StringRecord instead of a ByteRecord, and therefore [`Reader::read_record`](https://docs.rs/csv/1.0.0/csv/struct.Reader.html#method.read_record) instead of read_byte_record. This will give you easy access to Rust strings at the cost of UTF-8 validation but without the cost of allocating a new StringRecord for every record.

## Serde and zero allocation
In this section, we are going to briefly examine how we use Serde and what we can do to speed it up. The key optimization we’ll want to make is to—you guessed it—amortize allocation.

As with the previous section, let’s start with a simple baseline based off an example using Serde in a previous section:

```rust
extern crate csv;
extern crate serde;
 #[macro_use]
extern crate serde_derive;

use std::error::Error;
use std::io;
use std::process;

 #[derive(Debug, Deserialize)]
 #[serde(rename_all = "PascalCase")]
struct Record {
    country: String,
    city: String,
    accent_city: String,
    region: String,
    population: Option<u64>,
    latitude: f64,
    longitude: f64,
}

fn run() -> Result<u64, Box<Error>> {
    let mut rdr = csv::Reader::from_reader(io::stdin());

    let mut count = 0;
    for result in rdr.deserialize() {
        let record: Record = result?;
        if record.country == "us" && record.region == "MA" {
            count += 1;
        }
    }
    Ok(count)
}

fn main() {
    match run() {
        Ok(count) => {
            println!("{}", count);
        }
        Err(err) => {
            println!("{}", err);
            process::exit(1);
        }
    }
}
```

Now compile and run this program:

```shell
$ cargo build --release
$ time ./target/release/csvtutor < worldcitiespop.csv
2176

real    0m1.381s
user    0m1.367s
sys     0m0.013s
```

The first thing you might notice is that this is quite a bit slower than our programs in the previous section. This is because deserializing each record has a certain amount of overhead to it. In particular, some of the fields need to be parsed as integers or floating point numbers, which isn’t free. However, there is hope yet, because we can speed up this program!

Our first attempt to speed up the program will be to amortize allocation. Doing this with Serde is a bit trickier than before, because we need to change our Record type and use the manual deserialization API. Let’s see what that looks like:


```rust
#[derive(Debug, Deserialize)]
 #[serde(rename_all = "PascalCase")]
struct Record<'a> {
    country: &'a str,
    city: &'a str,
    accent_city: &'a str,
    region: &'a str,
    population: Option<u64>,
    latitude: f64,
    longitude: f64,
}

fn run() -> Result<u64, Box<Error>> {
    let mut rdr = csv::Reader::from_reader(io::stdin());
    let mut raw_record = csv::StringRecord::new();
    let headers = rdr.headers()?.clone();

    let mut count = 0;
    while rdr.read_record(&mut raw_record)? {
        let record: Record = raw_record.deserialize(Some(&headers))?;
        if record.country == "us" && record.region == "MA" {
            count += 1;
        }
    }
    Ok(count)
}
```

Compile and run:

```shell
$ cargo build --release
$ time ./target/release/csvtutor < worldcitiespop.csv
2176

real    0m1.055s
user    0m1.040s
sys     0m0.013s
```

This corresponds to an approximately 24% increase in performance. To achieve this, we had to make two important changes.

The first was to make our Record type contain &str fields instead of String fields. If you recall from a previous section, &str is a borrowed string where a String is an owned string. A borrowed string points to a already existing allocation where as a String always implies a new allocation. In this case, our &str is borrowing from the CSV record itself.

The second change we had to make was to stop using the [`Reader::deserialize`](https://docs.rs/csv/1.0.0/csv/struct.Reader.html#method.deserialize) iterator, and instead deserialize our record into a StringRecord explicitly and then use the [`StringRecord::deserialize`](https://docs.rs/csv/1.0.0/csv/struct.StringRecord.html#method.deserialize) method to deserialize a single record.

The second change is a bit tricky, because in order for it to work, our Record type needs to borrow from the data inside the StringRecord. That means that our Record value cannot outlive the StringRecord that it was created from. Since we overwrite the same StringRecord on each iteration (in order to amortize allocation), that means our Record value must evaporate before the next iteration of the loop. Indeed, the compiler will enforce this!

There is one more optimization we can make: remove UTF-8 validation. In general, this means using `&[u8]` instead of `&str` and ByteRecord instead of StringRecord:

```rust
#[derive(Debug, Deserialize)]
 #[serde(rename_all = "PascalCase")]
struct Record<'a> {
    country: &'a [u8],
    city: &'a [u8],
    accent_city: &'a [u8],
    region: &'a [u8],
    population: Option<u64>,
    latitude: f64,
    longitude: f64,
}

fn run() -> Result<u64, Box<Error>> {
    let mut rdr = csv::Reader::from_reader(io::stdin());
    let mut raw_record = csv::ByteRecord::new();
    let headers = rdr.byte_headers()?.clone();

    let mut count = 0;
    while rdr.read_byte_record(&mut raw_record)? {
        let record: Record = raw_record.deserialize(Some(&headers))?;
        if record.country == b"us" && record.region == b"MA" {
            count += 1;
        }
    }
    Ok(count)
}
```

Compile and run:

```shell
$ cargo build --release
$ time ./target/release/csvtutor < worldcitiespop.csv
2176

real    0m0.873s
user    0m0.850s
sys     0m0.023s
```

This corresponds to a 17% increase over the previous example and a 37% increase over the first example.

In sum, Serde parsing is still quite fast, but will generally not be the fastest way to parse CSV since it necessarily needs to do more work.

## CSV parsing without the standard library
In this section, we will explore a niche use case: parsing CSV without the standard library. While the csv crate itself requires the standard library, the underlying parser is actually part of the [`csv-core`](https://docs.rs/csv-core) crate, which does not depend on the standard library. The downside of not depending on the standard library is that CSV parsing becomes a lot more inconvenient.

The csv-core crate is structured similarly to the csv crate. There is a [`Reader`](https://docs.rs/csv-core/0.1.0/csv_core/struct.Reader.html) and a [`Writer`](https://docs.rs/csv-core/0.1.0/csv_core/struct.Writer.html), as well as corresponding builders [`ReaderBuilder`](https://docs.rs/csv-core/0.1.0/csv_core/struct.ReaderBuilder.html) and [`WriterBuilder`](https://docs.rs/csv-core/0.1.0/csv_core/struct.WriterBuilder.html). The csv-core crate has no record types or iterators. Instead, CSV data can either be read one field at a time or one record at a time. In this section, we’ll focus on reading a field at a time since it is simpler, but it is generally faster to read a record at a time since it does more work per function call.

In keeping with this section on performance, let’s write a program using only csv-core that counts the number of records in the state of Massachusetts.

(Note that we unfortunately use the standard library in this example even though csv-core doesn’t technically require it. We do this for convenient access to I/O, which would be harder without the standard library.)

```rust
extern crate csv_core;

use std::io::{self, Read};
use std::process;

use csv_core::{Reader, ReadFieldResult};

fn run(mut data: &[u8]) -> Option<u64> {
    let mut rdr = Reader::new();

    // Count the number of records in Massachusetts.
    let mut count = 0;
    // Indicates the current field index. Reset to 0 at start of each record.
    let mut fieldidx = 0;
    // True when the current record is in the United States.
    let mut inus = false;
    // Buffer for field data. Must be big enough to hold the largest field.
    let mut field = [0; 1024];
    loop {
        // Attempt to incrementally read the next CSV field.
        let (result, nread, nwrite) = rdr.read_field(data, &mut field);
        // nread is the number of bytes read from our input. We should never
        // pass those bytes to read_field again.
        data = &data[nread..];
        // nwrite is the number of bytes written to the output buffer `field`.
        // The contents of the buffer after this point is unspecified.
        let field = &field[..nwrite];

        match result {
            // We don't need to handle this case because we read all of the
            // data up front. If we were reading data incrementally, then this
            // would be a signal to read more.
            ReadFieldResult::InputEmpty => {}
            // If we get this case, then we found a field that contains more
            // than 1024 bytes. We keep this example simple and just fail.
            ReadFieldResult::OutputFull => {
                return None;
            }
            // This case happens when we've successfully read a field. If the
            // field is the last field in a record, then `record_end` is true.
            ReadFieldResult::Field { record_end } => {
                if fieldidx == 0 && field == b"us" {
                    inus = true;
                } else if inus && fieldidx == 3 && field == b"MA" {
                    count += 1;
                }
                if record_end {
                    fieldidx = 0;
                    inus = false;
                } else {
                    fieldidx += 1;
                }
            }
            // This case happens when the CSV reader has successfully exhausted
            // all input.
            ReadFieldResult::End => {
                break;
            }
        }
    }
    Some(count)
}

fn main() {
    // Read the entire contents of stdin up front.
    let mut data = vec![];
    if let Err(err) = io::stdin().read_to_end(&mut data) {
        println!("{}", err);
        process::exit(1);
    }
    match run(&data) {
        None => {
            println!("error: could not count records, buffer too small");
            process::exit(1);
        }
        Some(count) => {
            println!("{}", count);
        }
    }
}
```

And compile and run it:

```shell
$ cargo build --release
$ time ./target/release/csvtutor < worldcitiespop.csv
2176

real    0m0.572s
user    0m0.513s
sys     0m0.057s
```

This isn’t as fast as some of our previous examples where we used the csv crate to read into a StringRecord or a ByteRecord. This is mostly because this example reads a field at a time, which incurs more overhead than reading a record at a time. To fix this, you would want to use the Reader::read_record method instead, which is defined on csv_core::Reader.

The other thing to notice here is that the example is considerably longer than the other examples. This is because we need to do more book keeping to keep track of which field we’re reading and how much data we’ve already fed to the reader. There are basically two reasons to use the csv_core crate:

* If you’re in an environment where the standard library is not usable.
* If you wanted to build your own csv-like library, you could build it on top of csv-core.

## Closing thoughts
Congratulations on making it to the end! It seems incredible that one could write so many words on something as basic as CSV parsing. I wanted this guide to be accessible not only to Rust beginners, but to inexperienced programmers as well. My hope is that the large number of examples will help push you in the right direction.

With that said, here are a few more things you might want to look at:

* The [API documentation for the csv crate](https://docs.rs/csv/1.0.0/csv/index.html) documents all facets of the library, and is itself littered with even more examples.
* The [csv-index crate](https://docs.rs/csv-index) provides data structures that can index CSV data that are amenable to writing to disk. (This library is still a work in progress.)
* The [xsv command line tool](https://github.com/BurntSushi/xsv) is a high performance CSV swiss army knife. It can slice, select, search, sort, join, concatenate, index, format and compute statistics on arbitrary CSV data. Give it a try!
