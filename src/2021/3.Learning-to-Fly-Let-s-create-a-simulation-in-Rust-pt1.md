>* Learning to Fly: Let's create a simulation in Rust! (pt 1) 译文（学习飞行：用 Rust 创建一个模拟器 part1）
>* 原文链接：https://pwy.io/en/posts/learning-to-fly-pt1/
>* 原文作者：[Patryk27](https://github.com/Patryk27)
>* 译文来自：https://github.com/suhanyujie/article-transfer-rs/
>* 译者：[suhanyujie](https://github.com/suhanyujie)
>* 译者博客：[suhanyujie](https://ishenghuo.cnblogs.com/)
>* ps：水平有限，翻译不当之处，还请指正。
>* 标签：Rust, simulation,  genetic-algorithm, neural-network, rust, webassembly

系列文章：
* [Learning to Fly: Let's create a simulation in Rust! (pt 1)](https://pwy.io/en/posts/learning-to-fly-pt1/)
* [Learning to Fly: Let's create a simulation in Rust! (pt 2)](https://pwy.io/en/posts/learning-to-fly-pt2/)

In this series we’ll create a simulation of **evolution** using **neural network & genetic algorithm**.
>在这个系列中，我们将使用 **神经网络和遗传算法** 创建一个 **进化** 模拟器。

I’m going to introduce you to how a basic neural network and genetic algorithm works, then we’ll implement both in Rust and compile our application to **WebAssembly** to ultimately end up with:
>我会向你介绍一个基础的神经网络和遗传算法是如何工作的，然后使用 Rust 实现它们，并将应用编译到 **WebAssembly**，最后实现如下所示：

![Figure 1. https://pwy.io/en/projects/shorelark/](https://pwy.io/resources/learning-to-fly-pt1/intro-outcome.png)

>* This series is intended for **Rust beginners** - I’m assuming you know a thing or two about Rust and I’ll introduce you to rest of the concepts (such as neural networks) as we go.
>* 这个系列的文章主要面向 **Rust 初学者** - 我假定你已初步了解 Rust，之后我会向你介绍其他概念（如神经网络）。
>* No fancy mathematics or IT background is required.
>* 不需要熟练掌握花哨的数学和雄厚的 IT 背景。

This series will be divided into a few posts, roughly:
>这个系列将会分为若干篇文章，大概如下：

* 1.Introduction to the domain (what are we going to simulate, how does a neural network & genetic algorithm work),
* 1.领域说明（我们模拟什么，神经网络和遗传算法如何工作），
* 2.Implementing a neural network,
* 2.实现一个神经网络
* 3.Implementing a genetic algorithm,
* 3.实现一个遗传算法
* 4.Implementing eyes, brain, and the simulation itself.
* 4.实现眼睛、大脑和模拟器本身。

Due diligence: I’ll do my best to explain all the concepts, but if at any point you feel lost, feel free to take a look at this article’s last section - it contains links to external (mostly popular science) sources that might prove to be helpful in understanding the domain.
>尽职调查：我会尽我所能解释所有的概念，但如果你在任意时候有疑惑，可以看这篇文章的最后一节 —— 它包含了外部资源（主流科学）的一些链接，可能会帮助你理解对应的领域知识。

Curious? Hop on the bus, Gus, and onto the first chapter: [Design](https://pwy.io/en/posts/learning-to-fly-pt1/#design).
>好奇吗？快上车吧，Gus，我们进入第一章：[设计](https://pwy.io/en/posts/learning-to-fly-pt1/#design).

## [Design](https://pwy.io/en/posts/learning-to-fly-pt1/#design)

Let’s start by clearly defining our objective: what are we going to simulate, actually?
>我们先明确我们的目标：我们到底要模拟什么？

The overall idea is that we have a two-dimensional board representing a **world**:
>总体设想是我们有一个二维的**世界**：

![](https://pwy.io/resources/learning-to-fly-pt1/design-1.png)

This world consists of **birds** (hence the project’s original code name - _Shorelark_):
>这个世界由**鸟**构成（因此项目的仓库名用 —— _Shorelark_）：

![](https://pwy.io/resources/learning-to-fly-pt1/design-2.png)

... and **foods** (of an abstract kind, rich in protein & fiber):
>... 以及**食物**（抽象类实体，富含蛋白质和纤维）：

![](https://pwy.io/resources/learning-to-fly-pt1/design-3.png)

Each bird has their own **vision**, allowing them to locate the food:
>每只鸟有它们的**视界**，它们可以自己寻找食物：

![](https://pwy.io/resources/learning-to-fly-pt1/design-4.png)

... and a **brain** that controls bird’s body (i.e. speed and rotation).
>... 并且由**大脑**控制鸟的身体（比如速度和旋转）。

Our magic touch will lay in the fact that instead of hard-coding our birds to some specific behavior (e.g. "go to the nearest food in your eyesight"), we’ll take a more intriguing route:
>我们的神奇之处在于，我们没有将鸟类硬编码为具有特定的行为的实体（例如：去定位你视线范围内最近的食物），我们会走一条更有趣的路线：

We’ll make our birds able to **learn** and **evolve**.
>我们要确保鸟实体能够**学习**和**进化**。

## 大脑

If you squint your eyes hard enough, you’ll see that a brain is nothing but a **function** of some inputs to some outputs, e.g.:
>如果你把眼睛眯得足够紧，你会发现大脑只不过是一个具备输入和输出的**函数**，例如：

![](https://pwy.io/resources/learning-to-fly-pt1/brain-1.png)

>You’re a precious mathematical formula, remember that.
>你是一个珍贵的数学公式，记住这点：

Since our birds will have only one sensory input, their brains can be then approximated as:
>由于鸟类只有一种感觉输入，它们的大脑可以视为：

![](https://pwy.io/resources/learning-to-fly-pt1/brain-2.png)

Mathematically, we can represent this function’s input (i.e. biological eye) as a list of numbers, with each number (i.e. biological _photoreceptor_) describing _how close_ the nearest object (i.e. food) is:
>数学角度看，我们可以将这个函数的输入（如生物的眼睛）表示为一组数，每个数字（生物感光器）描述周围的物体（如食物）与其之间的距离：

![](https://pwy.io/resources/learning-to-fly-pt1/brain-3.png)

_(0.0 - 视线范围内没有物体, 1.0 - 在我们的右侧有一个对象.)_

>Our birds won’t see color, but that’s just for simplicity - you could use e.g. [raytracing](https://raytracing.github.io/books/RayTracingInOneWeekend.html) to make the eyes more realistic.
>我们的鸟不能识别颜色，但这是为了简化 —— 你可以使用 [raytracing](https://raytracing.github.io/books/RayTracingInOneWeekend.html) 之类的库使眼睛的实现更接近真实。

As for the output, we’ll make our function return a tuple of `(Δspeed, Δrotation)`.
>至于输出，我们使函数返回一个元组 `(Δspeed, Δrotation)`。

For instance, a brain telling us `(0.1, 45)` will mean "body, please increase our speed by `0.1` units and rotate us `45` degrees clockwise", while `(0.0, 0)` will mean "body, please keep our course steady".
>例如，大脑告诉我们 `(0.1, 45)` 意味着“身体，请给我们的速度增加 `0.1` 个单位，并顺时针旋转 45 度”，而如果大脑接收到的是 `(0.0, 0)`，则意味着“身体，请保持我们的航向一直向前”。 

>* It’s important that we use relative values (so `delta speed` and `delta rotation`), as our brain won’t be aware of its own location & rotation respective to the world - passing that information would increase our brain’s complexity for no real benefit.
>* 重要的是我们要使用相对值（即 `delta speed` 和 `delta rotation`），因为我们的大脑不会意识到自己的位置和旋转的角度 —— 传递这些信息只会增加大脑的复杂度而没有其他益处。

Finally, let’s address the elephant in the room: so a brain is basically `f(eyes)`, right? But how do we find out what actually follows the equals sign?
>最终，让我们来谈谈[“房间里的大象”](https://www.jianshu.com/p/981ad4a6fdc6)：所以大脑基本上可以表示为 `f(eyes)`，对吧？但我们如何知道下方的等式中等号后到底是什么？

```
f(eyes) = what?
```

## 神经网络：简介

As a fellow human, you are might be aware that brains are made of neurons connected via synapses:
>作为一个人类同胞，你可能知道大脑由神经元通过突触连接而成：

![](https://pwy.io/resources/learning-to-fly-pt1/nn-1.png)

_Figure 2. 我没有按比例绘制神经元_

Synapses carry electric and chemical signals between neurons, while neurons "decide" whether given signal should be propagated further or stopped; eventually this allows for people to recognize letters, eat brussels sprouts, and write mean comments on Twitter.
>突触在神经元之间传递电信号和化学信号，而神经元“决定”已知的信号是应该继续传递还是停止传递；最终，人们可以识别字母，吃球芽甘蓝，以及在 Twitter 发表评论。

Due to their [inherent complexity](https://en.wikipedia.org/wiki/Biological_neuron_model), biological neural networks are not among the easiest to simulate (one could argue that neurons are thus not [Web Scale](https://www.youtube.com/watch?v=b2F-DItXtZs)), which made some smart people invent a class of mathematical structures called artificial neural networks, which allow to approximate - with a pinch of salt - brain-like behavior using math.
>由于其固有的[复杂性](https://en.wikipedia.org/wiki/Biological_neuron_model)，我们不太容易去模拟生物神经网络的神经元（可以说没有达到[网络规模](https://www.youtube.com/watch?v=b2F-DItXtZs)），这使得一些聪明的人发明了一种人工神经网络的数学结构，使得可以用数学来近似地表示大脑的行为。

Artificial neural networks (which I’m going to call just neural networks) are prominent at **generalizing** over datasets (e.g. learning how a cat looks like), so they found their use in face recognition (e.g. for cameras), language translation (e.g. for [GNMT](https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation)), and - in our case - to steer colorful pixels for a handful of reddit karma.
>人工神经网络（我称之为神经网络）以**归纳**数据集（如学习一只猫是什么样子）著称，所以它可用于面部识别（如相机），语言翻译（如 [GNMT](https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation)），而在我们的场景中，就是给一些 reddit 社区成员引导彩色像素。

The particular kind of network we’ll be focusing on is called `feedforward neural network` (FFNN)…​
>我们将上面所描述的特殊网络称之为“前馈神经网络”（FFNN）…​

>Cool bear’s hot tip: FFNNs are sometimes called [multilayer perceptrons](https://en.wikipedia.org/wiki/Multilayer_perceptron) and they are one of the building [blocks of convolutional neural networks](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53), such as [DeepDream](https://en.wikipedia.org/wiki/DeepDream).
>酷熊的温馨提示：FFNNs 有时被称为[多层感知器](https://en.wikipedia.org/wiki/Multilayer_perceptron)，并且它也是一种[卷积神经网络模块](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)，例如 [DeepDream](https://en.wikipedia.org/wiki/DeepDream)。

... 它看起来类似于这样:

![](https://pwy.io/resources/learning-to-fly-pt1/nn-2.png)

_Figure 3. 多层感知器（MLP），也称 FFNN _

This is a layout of an FFNN with **five synapses** and **three neurons**, all organized in **two layers**: the **input layer** (on the left side) and the **output layer** (on the right side).
>这是一个多层感知器的布局结构，又**五个突出**和**三个神经元**，**两层**的所有组织：**输入层**（在左侧）和**输出层**（在右侧）。

There may also exist layers in-between, in which case they are called hidden layers - they improve the network’s ability to understand the input data (think: the bigger the brain, the "more abstraction" it might understand, to a certain degree).
>也有可能存在中间层，在这种情况下，它被称为隐藏层 —— 它可以提高网络输入数据的能力（可以想象一下：大脑越大，在一定程度上，理解抽象的能力更强）。

>A [similar process](https://www.youtube.com/watch?v=rA5qnZUXcqo) happens inside your own [visual cortex](https://en.wikipedia.org/wiki/Visual_cortex), too.
>[类似的过程](https://www.youtube.com/watch?v=rA5qnZUXcqo)也发生在你的[视觉皮层](https://en.wikipedia.org/wiki/Visual_cortex)中。

Contrary to biological neural networks (which piggyback on electric signals), FFNNs work by accepting some **numbers** at their input and propagating (_feedforwarding_) those numbers layer-by-layer across the entire network; numbers that appear at the last layer determine network’s answer.
>与生物神经网络（基于电信号）相反，FFNNs 的工作原理是在输入时接受一些**数字**，然后再整个网络中一层一层地传播这些数字；当它们到达最后一层时，就达成了整个神经网络的目的。

For instance, if you fed your network with raw pixels of a picture, you might’ve got a response saying:
>例如，如果你将一张图片的原始像素输入你的网络，你可能得到这样的结果：

* `0.0` - this picture does not contain an orange cat eating lasagna,
* `0.0` - 这张照片里没有在吃千层面的橙色猫，
* `0.5` - this picture might contain an orange cat eating lasagna,
* `0.5` - 这张照片可能会有一只在吃千层面的橙色猫，
* `1.0` - this picture certainly contains an orange cat eating lasagna.
* `1.0` - 这张照片里肯定有一只在吃千层面的橙色猫。

It’s also possible for a network to return _many values_ (the number of output values is equal to the number of neurons in the output layer):
>网络也有可能返回 _多个结果值_（输出的值的数量等同于输出层神经元的数量）：

* `(0.0, 0.5)` - this picture _does not_ contain an orange cat, but _might_ contain a lasagna,
* `(0.0, 0.5)` - 这张照片 _没有_ 橙色的猫，但 _可能_ 有一个千层面，
* `(0.5, 0.0)` - this picture _might_ contain an orange cat, but _does not_ contain a lasagna.
* `(0.5, 0.0)` - 这张照片 _可能_ 有橙色的猫，但 _没有_ 千层面。

The meaning of input and output numbers is up to you - in this case we’ve simply imagined that there exists some neural network behaving this way, but in reality it’s on you to prepare so-called **training dataset** ("given this picture, you should return 1.0", "given that picture, you should return 0.0").
>输入数值和输出数值的意义因你而定 —— 在这种情况下我们只能简单地想象，在现实中，存在一种神经网络的行为方式，它依赖于你准备的**训练数据集**（“给定一张照片，返回 1.0”，“给定另一张照片，你应该返回 0.0”）。

>You might’ve as well created a network that, say, [identifies mature apples](https://www.researchgate.net/publication/320662740_Identification_and_counting_of_mature_apple_fruit_based_on_BP_feed_forward_neural_network) - sky’s the limit.
>你也可以创建一个网络，比如，[识别成熟的苹果](https://www.researchgate.net/publication/320662740_Identification_and_counting_of_mature_apple_fruit_based_on_BP_feed_forward_neural_network) - sky’s the limit

Having the general overview of FFNNs in mind, let’s now take the next major step and learn about the magic allowing for all of this to happen.
>大体上对 FFNNs 有了了解，现在我们进入下一个关键步骤，去理解让这些可能性得以发生的魔法。

## 神经网络：深入

FFNNs lean on two building blocks: neurons and **synapses**.
>FFNNs 依赖于两个模块：神经元和**突触**。

A **neuron** (usually represented with a circle) accepts some input values, processes them, and returns some output value; each neuron has at least one input and at most one output:
>一个**神经元**（通常用圆表示）接受一些输入，处理他们，并返回一些输出值；每个神经元至少一个输入，并且最多一个输出：

![](https://pwy.io/resources/learning-to-fly-pt1/nn-3.png)

_Figure 4. 有三个突触的单个神经元_

Additionally, each neuron has a **bias**:
>另外，每个神经元都有一个**偏差**：

![](https://pwy.io/resources/learning-to-fly-pt1/nn-4.png)

_Figure 5. A single neuron with three synapses and annotated bias value_
_Figure 5. 一个具有三个突触和一个已经标注出来的偏差值的单个神经元_

Bias is like a neuron’s `if` statement - it allows for a neuron to stay inactive (return an output of zero) unless the input is strong (high) enough. Formally we’d say that bias allows to regulate neuron’s **activation threshold**.
>偏差类似于神经元的 `if` 语句 —— 它允许神经元保持非活动状态（返回 0 的输出）除非输入足够强（高）。确切地说，偏差可以调节神经元的**激活阈值**。

Imagine you’ve got a neuron with three inputs, with each input determining whether it sees a lasagna (`1.0`) or not (`0.0`) - now, if you wanted to create a neuron that’s activated when it sees at least two lasagnas, you’d simply create a neuron with a bias of `-1.0`; this way your neuron’s "natural" state would be `-1.0` (inactive), with one lasagna - `0.0` (still inactive), and with two - `1.0` (active, voilà).
假如你有一个神经元，它有三个输入，每个输入都能决定它看到的是千层面（`1.0`）还是看不到（`0.0`） —— 现在，假如你创建了一个神经元，当它看到两个以上的千层面时就会被激活，你只需创建一个偏差为 `-1.0` 的神经元，这样一来，神经元的“自然”状态就会是 `-1.0`（不活动），看到一个千层面时 `0.0`（仍然是不活动），当看到两个时 —— `1.0`（活跃，voilà）。

>If my lasagna metaphor doesn’t appeal to you, you might find [this math-oriented explanation](https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks) more helpful.
>如果我的千层面的比喻还不能让你理解，你可以看看这个[面向数学的解释](https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks)，也许对你有帮助。

Apart from neurons, we’ve got synapses - a **synapse** is like a wire that connects one neuron’s output to another neuron’s input; each synapse is of certain **weight**:
>除了神经元，我们还有突触 —— 突触就像一根电线，连接一个神经元的输出和另一个神经元的输入；每个突触都有一定的**权重**：

![](https://pwy.io/resources/learning-to-fly-pt1/nn-5.png)

_Figure 6. A single neuron with three synapses with annotated weights_
_Figure 6. _有三个突触的神经元，突触上已注明权重_

A weight is a factor (hence the `x` before each number, to emphasize its multiplicative nature), so a weight of:
>权重是一种因子（因此每个数字都有 `x`，用于与其相乘）因此权重可以表示为：

* `0.0` means that a synapse is effectively dead (it doesn’t pass any information from one neuron into the another),
* `0.0` 意味着突触实际上已经死亡（它不会将任何信息从一个神经元传递到另一个神经元），
* `0.3` means that if neuron A returns `0.7`, neuron B will receive `0.7 * 0.3 ~= 0.2`,
* `0.3` 意味着如果神经元 A 返回 `0.7`，那么神经元 B 将接收到 `0.7 * 0.3 ~= 0.2`，
* `1.0` means that a synapse is effectively passthrough - if neuron A returns `0.7`, neuron B will receive `0.7 * 1.0 = 0.7`.
* `1.0` 表示神经元高效地传递 —— 如果神经元 A 输出 `0.7`，那么神经元 B 将接收到 `0.7 * 1.0 = 0.7`。

Having all this knowledge in mind, let’s go back to our network and fill-in missing weights & biases with some random numbers:
>有了这谢谢知识，我们回到前面说到的网络，用一些随机数来填补缺失的权重和偏差：

![](https://pwy.io/resources/learning-to-fly-pt1/nn-6.png)

What a beauty, isn’t it?
>很漂亮，对吧？

Let’s see what it thinks of, say, `(0.5, 0.8)`:
>我们看看它是怎么“思考”的，比如 `(0.5, 0.8)`：

![](https://pwy.io/resources/learning-to-fly-pt1/nn-7.png)

To reiterate, we’re interested in the output value of the rightmost neuron (that’s our output layer) - since it depends on two previous neurons (the ones from the input layer), we’re going to start with them.
>重申一下，我们的重点是最右边的神经元（即输出层） —— 因为它依赖于前面两个神经元（输入层的神经元），我们将从它们开始。

Let’s focus on the top-left neuron first - to calculate its output, we start by computing a **weighted sum** of all its inputs:
>我们先关注左上角的神经元 —— 为了计算它的输出，我们首先计算它所有输入的加权和：

```
0.5 * 0.2 = 0.1
```

... then, we add the bias:
>... 然后，将偏差相加：

```
0.1 - 0.3 = -0.2
```

... and clamp this value through so-called activation function; activation function limits neuron’s output to a predefined range, simulating the `if`-like behavior.
>... 通过所谓的激活功能来控制这个值；激活函数将神经元的输出限制在一个预先所设定的范围内，模拟类似于“假设”的行为。

The simplest activation function is rectified linear unit (`ReLU`), which is basically `f32::max`:
>最简单的激活函数就是矫正线性单元（`ReLU`），基本上就是 `f32::max`：

![](https://pwy.io/resources/learning-to-fly-pt1/nn-8.png)

>* Another popular activation function is `tanh` - its graph looks slightly different (like an `s`) and it’s got [different properties](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks).
>* 另一个流行的激活函数是 `tanh` —— 它的图形看起来有点不同（类似于 `s`），它有[不同的属性](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks)。

>* **Activation function affects network’s input and output** - e.g. `tanh` forces a network to work on numbers from range `<-1.0, 1.0>` instead of `<0.0, +inf>`, as compared to `ReLU`.
>* **激活函数影响网络的输入和输出** —— 比如，与 `ReLU` 相比，`tanh` 强制网络工作在 `<0.0, +inf>` 范围内，而非 `<0.0, +inf>`。

As you can see, when our weighted-sum-with-a-bias is lower than zero, the neuron’s output will be `0.0` - and that’s exactly what happens to our current output:
>如你所见，当加权偏差和小于 0 时，神经元的输出将是 `0.0` —— 这正是当前输出的实际情况：

```
max(-0.2, 0.0) = 0.0
```

Nice; now let’s do the bottom-left one:
>太好了；现在让我们来做左下角的那个：

```
# 权重和：
0.8 * 1.0 = 0.8

# 偏差：
0.8 + 0.0 = 0.8

# 激活函数：
max(0.8, 0.0) = 0.8
```

At this point we’ve got the input layer completed:
>现在我们已经完成了输入层：

![](https://pwy.io/resources/learning-to-fly-pt1/nn-9.png)

... which heads us towards the last neuron:
>... 现在转向最后一个神经元：

```
# 权重和：
(0.0 * 0.6) + (0.8 * 0.5) = 0.4

# 偏差：
0.4 + 0.2 = 0.6

# 激活函数：
max(0.6, 0.0) = 0.6
```

... and the network’s output itself:
>... 该网络输出还是它本身的值：

```
0.6 * 1.0 = 0.6
```

Voilà - for the input of `(0.5, 0.8)`, our network responded `0.6`.
> Voilà - 也就是说输入是 `(0.5, 0.8)` 时，我们的网络就会响应 `0.6`。

_(since it’s just an exercise on a totally made-up network, this number doesn’t mean anything - it’s just some output value.)_

_（因为它只是一个完全虚构的网络练习，数字本身没有任何意义 —— 只是一些输出值）_

Overall, that’s one of the simplest FFNNs possible - given appropriate weights, it’s able to solve [the XOR problem](https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b), but probably lacks computational capacity to steer a bird.
>总的来说，这可能是一种最简单的 FFNNs —— 给定适当的权重，它能够解决 [XOR 问题](https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b)，但缺乏控制鸟的计算能力。

More complex FFNNs, such as this one:
>复杂一些的 FFNNs，如下所示：

![](https://pwy.io/resources/learning-to-fly-pt1/nn-10.png)

... work exactly the same way: you just go left-to-right, neuron-by-neuron, computing the outputs, until you squeeze all the numbers from the output layer.
... 工作原理上是类似的：你只需要从左到右，一个神经元接着一个神经元地进行计算输出，直到你从输出层得到所有的数字。

_(on that particular diagram some of the synapses overlap, but it doesn’t mean anything: it’s just that representing multidimensional graphs on a flat screen is unfortunate.)_
_（在上方的图中，一些突触重叠，但这并没有什么特别的含义：只是在平面上不太好绘制多维图形而已）_

At this point you might begin to wonder "wait, how do we know our network’s weights?", and for that I’ve got a simple answer:
>在这一点上，你可能会想到：“等等，我们如何知道网络的权重呢？”，对此，我有个简单的答案：

**we randomize them! ❤️️**
>**我们直接取随机值吧！❤️️**

If you’re accustomed to deterministic algorithms (bubble sort, anyone?), this might feel non-diegetic to you, but it’s the way things go for networks containing more than a few neurons: you cross your fingers, randomize the initial weights, and work with what you got.
>如果你已经习惯使用确定性的算法（冒泡算法等等），这可能对你不太直白，但这却是多个神经元网络的运行方式：将你的手指交叉，随机分配一些初始的权重值，并根据你的需要进行工作。

Notice I said initial weights - having some non-zero weights in place, there are certain algorithms that you can apply on your network to improve it (so, essentially, to teach it).
>注意我说的初始权重值 —— 有一些地方是非零权重值，有一些特定的算法，你可以应用到你的网络上改进它（所以，本质上就是你训练它）。

One of the most popular "teaching" algorithms for FFNNs is [backpropagation](https://www.youtube.com/watch?v=tIeHLnjs5U8):
>FFNNs 的最通用的“训练”方f法之一是[反馈传播 （backpropagation）](https://www.youtube.com/watch?v=tIeHLnjs5U8):

You show your network lots (think: hundredths of thousands) of examples in the form of "for this input, you should return that" (think: "for this picture of dakimakura, you should return pillow"), and backpropagation slowly adjusts your network’s weights until it gets the answers right.
>你以“这种输入，应该对应那个输出”的形式向你的网络展示大量（考虑一下成百上千个例子）的例子，（考虑：“输入 dakimakura 图片，你应该返回枕头”），然后 backpropagation 慢慢那调整你的网络权重值，直至得到正确答案。

>* Or not - a network might get stuck at a [local optimum](https://en.wikipedia.org/wiki/Local_optimum) and "just" stop learning.
>* 否则 —— 网络可能陷入[局部最优](https://en.wikipedia.org/wiki/Local_optimum)，然后“就”停止学习。
>* Also, if you ever find yourself doing a neural network crossword, remember that backpropagation is an example of [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning).
>* 此外，如果你发现自己在做神经网络的天子游戏，请记住 backpropagation 只是[监督学习](https://en.wikipedia.org/wiki/Supervised_learning)的 一个例子而已。

Backpropagation is a great tool if you have a rich set of labeled examples (such as photos or statistics), and that’s why it doesn’t fit our original assumption:
>如果你有一组丰富的标记示例（如照片或统计数据）， Backpropagation 是一个很棒的工具，这就是为什么它不符合我们最初的假设：

We ain’t no statisticians, the world is a cruel place, so we want for our birds to figure out all the learning on their own (contrary to being given concrete examples of "for this vision, go left", "for this vision, go right").
>我们不是统计学家，而世界是一个残酷的地方，所以我们向让我们的鸟类自己去学习所有的东西（相反地，给它们一些具体地例子，“为了这个远景，向左走”，“为了那个目标，向右走”）。

Solution?
>解决方法呢？

~~biology~~ genetic algorithms and the magic of [large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers)
>~~生物学~~ 遗传算法和[大数字](https://en.wikipedia.org/wiki/Law_of_large_numbers)的魔力。

## Genetic algorithm: Introduction
>遗传算法：介绍

To recap, from the mathematical point of view our underlying problem is that we have a function ([represented](https://en.wikipedia.org/wiki/Universal_approximation_theorem) using a neural network) that’s defined by a whole lot of **parameters**:
>回顾一下，从数学角度看，我们潜在的问题是有一类函数（它[代表](https://en.wikipedia.org/wiki/Universal_approximation_theorem)了神经网络），它的定义中有大量的**参数**：

![](https://pwy.io/resources/learning-to-fly-pt1/ga-1.png)

_(I didn’t bother to draw all the weights, but I hope you get the point - there’s a lot of them.)_

Had we represented each parameter with a single-precision floating-point number, a network of mere 3 neurons and 5 synapses could be defined in so many different combinations…​
>如果我们用一个单精度浮点数来表示每个参数，那么仅由 3 个神经元和 5 个突触就能定义很多个不同的组合...

```
(3.402 * 10^38) ^ (3 + 5) ~= 1.8 * 10^308
```

_[(how-many-floating-point-numbers-are-there)](https://jameshoward.us/2015/09/09/how-many-floating-point-numbers-are-there/)_

... that the universe would sooner meet its [ultimate fate](https://en.wikipedia.org/wiki/Heat_death_of_the_universe) than we’d be done checking them all; we certainly need to be smarter!
>...宇宙很快就能遭遇它的[最终命运](https://en.wikipedia.org/wiki/Heat_death_of_the_universe)，我们不必逐个检查它们，而是要让它更加智能！

>* All the possible sets of parameters are called a **search space**.
>* 所有可能的参数集合成为**搜索空间**。

Since iterating through our search space looking for the single best answer is off the table, we can focus on a much simpler task of finding a list of _suboptimal_ answers.
>由于遍历搜索空间寻找单个最佳答案不在大纲中，所以我们可以把重点放在一个简单一点的任务上，也就是寻找 _次优_ 答案列表。

And in order to do that, we must **dig deeper**.
>为了做到这一点，我们必须**深挖**。

## Genetic algorithm: Deep dive

This is a wild carrot together with its domesticated form:
>这是一个野生胡萝卜和它的驯化后的样子：

![](https://pwy.io/resources/learning-to-fly-pt1/carrot.jpg)

This domesticated, widely known form didn’t appear out of blue - it’s an outcome of hundredths of years of [selective breeding](https://en.wikipedia.org/wiki/Selective_breeding) with certain factors - like taproot’s texture or color - in mind.
>这种驯化了的、广为人知的品种不是突然出现的 —— 它是经过很多年的[选择性育种](https://en.wikipedia.org/wiki/Selective_breeding)以及经历了很多因素后（如主根的质地和颜色）的结果。

Wouldn’t it be awesome if we could do a similar thing with our neural brains? If we just, like, created a bunch of random birds and selectively bred the ones who seemed the most prominent…​
>如果我们能对大脑做类似的事情，那岂不是很棒？如果我们只是创造一群鸟，并且它们会选择性地繁殖那些看起来杰出的鸟...

**hmmm**

As it turns out, we’re not the first to stumble upon this idea - there already exists a widely researched branch of computer science called [evolutionary computation](https://en.wikipedia.org/wiki/Evolutionary_computation) that’s all about solving problems "just the way nature would do".
>事实证明，我们不是第一个有这种想法的人 —— 有一个被广泛研究的计算机科学分支，叫[进化计算](https://en.wikipedia.org/wiki/Evolutionary_computation)，它的核心思想是“按照自然选择的方式”解决问题。

Out of all the evolutionary algorithms, the concrete subclass we’ll be studying is called [genetic algorithm](https://en.wikipedia.org/wiki/Genetic_algorithm).
>在所有的进化算法中，我们将要学习的具体子类叫做[遗传算法](https://en.wikipedia.org/wiki/Genetic_algorithm)。

>* Similarly as with neural networks, there’s no the genetic algorithm - it’s a variety of different algorithms; so to avoid burning the midnight oil, we’ll take a look at how things work _generally_.
>* 类似于神经网络，不只是有遗传算法 —— 还有很多不同的其他算法；因此，为了避免午夜开车问题，我们将看看一切是如何 _正常_ 运作的。

Starting top-bottom, a genetic algorithm starts with a **population**:
>从上到下，遗传算法是从一个**种群**开始的：

![](https://pwy.io/resources/learning-to-fly-pt1/ga-2.png)

A population is built from **individuals** (sometimes called **agents**):
>人口都是由**个体**（有时称为 **agents**）组成的：

![](https://pwy.io/resources/learning-to-fly-pt1/ga-3.png)

An **individual** (or an **agent**) is a single _possible solution_ to given problem (a population is thus a set of some possible solutions).
>**个体**（或 **agent**）是一个单一的解决方案（因此，population 则是一些解决方案的集合）。

In our case, each individual will model a brain (or an entire bird, if you prefer to visualise it this way), but generally it depends on the problem you’re tackling:
>在我们的例子中，每个个体都将模拟一个大脑（或者是整只鸟，如果你能这样想的话），但这通常取决于你所处理的问题：

* If you were trying to, say, [evolve an antenna](https://en.wikipedia.org/wiki/Evolved_antenna), a single individual would be a single antenna.
* 比如说，如果你想[进化一根天线](https://en.wikipedia.org/wiki/Evolved_antenna)，那么一个个体就是一个天线。
* If you were trying to, say, [evolve a query plan](https://www.postgresql.org/docs/8.3/geqo-intro2.html), a single individual would be a single query plan.
* 例如，你正在尝试[演进一个查询计划](https://www.postgresql.org/docs/8.3/geqo-intro2.html)，单个个体将成为单个查询计划。

>* An individual represents some solution, but not necessarily the best or even a remotely desirable one.
>* 一个个体代表了某种解决方案，但不一定是最好的，甚至是不理想的。

An individual is built from **genes** (collectively named **genome**):
>个体由**基因**（统称为**基因组**）构成：

![](https://pwy.io/resources/learning-to-fly-pt1/ga-4.png)

_Figure 7. A genome represented with neural network’s weights; a genome might be a list of numbers, a graph or anything else that is able to model a solution to the problem_
>图 7。用神经网络权值表示的基因组；基因组可能是一列数字、一张图表或任意能够为问题的解决方案提供建模的元素。


A **gene** is a single parameter that’s being evaluated and tuned by the genetic algorithm.
>基因是一个由遗传算法评估和调整的单一参数。

In our case, each gene will be simply a neural network’s weight, but representing problem’s domain isn’t always this straightforward.
>在我们的例子中，每个基因只是一个神经网络的权重，但表示问题的领域并不那么简单。

For instance, if you were trying to [help a fellow salesman](https://en.wikipedia.org/wiki/Travelling_salesman_problem), where the underlying problem isn’t based on neural networks at all, a single gene could be a tuple of `(x, y)` coordinates determining a part of a salesman’s journey (consequently, an individual would then describe a salesman’s entire path):
>例如，你正在试图[帮助一个推销员](https://en.wikipedia.org/wiki/Travelling_salesman_problem)，这种情况下，根本的问题不是基于神经网络，一个基因可能是一个 `(x, y)` 的元组坐标确定推销员的一部分路径（因此，整个个体则描述推销员的整个实体路径）：

![](https://pwy.io/resources/learning-to-fly-pt1/ga-5.png)

_Figure 8. A hypothetical approach to the travelling salesman problem - each box represents a probable, suggested path for the salesman to travel_
_图 8. 推销员旅行问题的一个假设方法 —— 每个盒子代表推销员旅行的一个可能路径_

Now, let’s say we’ve got a random population of fifty birds - we pass them to a genetic algorithm, what happens?
>现在，假设我们有随机的 50 只鸟的种群，我们把它们传递给遗传算法，会发生什么？

Similarly as with selective breeding, genetic algorithm starts by **evaluating** each of the individuals (each of the possible solutions) to see which are the best among the current population.
>和选择性育种类似，遗传算法通过**评估**每个个体（每一个可能性）开始，看看当前种群中哪一个是最好的。

Biologically, this is an equivalent of taking a stroll to your garden and checking which carrots are the orangest and the yummiest.
>从生物学角度来说，这就相当于你在花园散步，看看哪种胡萝卜颜色最深，最美味。

Evaluation happens using so-called **fitness function** that returns a **fitness score** quantifying how good a particular individual (so a particular solution) is:
>使用所谓的**适应度函数*进行评估**，返回一个**适应度分数**来量化个体的特征（即一个特定的解决方案）的好坏：

![](https://pwy.io/resources/learning-to-fly-pt1/ga-6.png)

_Figure 9. An example of a fitness function that quantifies carrots by their taproot’s color and radius_
_图 9。一个适应度函数的例子，通过胡萝卜主根的颜色和半径来量化胡萝卜的好坏_

Creating a [usable](https://www.youtube.com/watch?v=7J-DfS52bnI) fitness function is one of the hardest tasks when it comes to genetic algorithms, as usually there are many metrics by which an individual can be measured.
>当涉及到遗传算法时，创建一个[可用的](https://www.youtube.com/watch?v=7J-DfS52bnI)适应度函数是最困难的任务之一，因为通常有许多指标可以衡量一个个体。

(even our imaginative carrot has at least three metrics: taproot’s color, radius, and taste, that have to be squashed into a single number.)
>（即使是我们想象中的胡萝卜也至少有三个参数：主根的颜色、半径和味道，这些都必须压缩成一个数字表示。）

Fortunately, when it comes to birds, we don’t have much to choose from anyway: we’ll just say that a bird is as good as the amount of food it ate during the course of current **generation**.
>幸运的是，当涉及到鸟类时，我们没有太多选择：只能简单地说，一只鸟和它在生存期间所吃的食物数量一样好。

A bird who ate `30` foods is better than the one who ate just `20`, simple as that.
>吃 `30` 种食物的鸟比吃 `20` 种食物的鸟要好，就这么简单。

>* Negating a fitness function makes a genetic algorithm return the worst solutions instead of the best ones; just an amusing trick to remember for later.
>* 否定适应度函数会使遗传算法返回最坏的解而不是最好的解；只是一个有趣的规则，待会细说。

Now, the time has come for the genetic algorithm’s crème de la crème: **reproduction**!
>现在，是时候进入遗传算法 crème de la crème 的实现了：**繁殖**！

Broadly speaking, reproduction is the process of building a new (hopefully - slightly improved) population starting from the current one.
>广义地说，繁殖是在现有地种群基础上繁殖新的（希望 - 慢慢改善）个体的过程。

It’s the mathematical equivalent of choosing the tastiest carrots and planting their seeds.
>这在数学上相当于选择最美味的胡萝卜，然后，把它们的种子种下去。

What happens is that the genetic algorithm chooses two individuals at random (prioritizing the ones with the higher fitness scores) and uses them to produce two new individuals (a so-called **offspring**):
>主要发生的是，遗传算法随机选择两个个体（优先选择适应性得分较高的），并使用它们产生两个新的个体（所谓的**后代**）：

![](https://pwy.io/resources/learning-to-fly-pt1/ga-7.png)

Offspring is produced by taking genomes of both parents and performing [crossover](https://en.wikipedia.org/wiki/Chromosomal_crossover) & [mutation](https://en.wikipedia.org/wiki/Mutation) on them:
>后代的产生是将双亲的基因组进行[交叉](https://en.wikipedia.org/wiki/Chromosomal_crossover)以及[突变](https://en.wikipedia.org/wiki/Mutation)：

![](https://pwy.io/resources/learning-to-fly-pt1/ga-8.png)

>* **Crossover** allows to mix two different gnomes to get an approximate in-between solution, while **mutation** allows to discover new solutions that weren’t present in the initial population.
>* **交叉**可以让两个不同的侏儒获得近似的中间解决方案，而**突变**可以得到种群尚未出现的新基因。

Both newly-spawned individuals are pushed into the pool of `new population` and the process starts over until the entire new population is built; the current population then gets discarded and the whole simulation starts over on this new (hopefully improved!) population.
>两个新产生的个体都会被放到 `new population` 中，然后重复这个过程，直到整个新种群形成；而现有的种群就会被淘汰，整个模拟过程又基于新的种群继续淘汰、生成。

As you can see, there’s a lot of **randomness** in the process: we start with a random population, we randomize how the genes are being distributed…​ so…​
>正如你看到的，在这个过程中有很多的随机性：我们从一个随机的种群开始，我们随机进行基因分布。。。所以。。。

this cannot actually work, can it?
>这实际上行不通，不是吗？

## The Code

Let’s end this post with a cliffhanger:
我们以一个悬念来结束这篇文章：

```shell
$ mkdir shorelark
```

Can you guess why I didn’t use `cargo new`?
>你能猜一猜，我为啥不用 `cargo new`？

In the second part we’ll implement a working, bare-bones feed-forward neural network - until then!
>在第二部分中，我们将实现一个可以工作的，基础的前馈神经网络 —— 到那时你就懂了！

## Sources

Here are some of the sources that I’ve personally found useful while learning about topics presented in this article:
>以下是我个人在学习本文的主题时发现的一些资料：

**神经网络：**
* [YouTube, 3Blue1Brown - 神经网络是什么？](https://www.youtube.com/watch?v=aircAruvnKk)
* [YouTube, Vsauce - Stilwell 大脑](https://www.youtube.com/watch?v=rA5qnZUXcqo)

**遗传算法：**

* [YouTube, Jeremy Fisher - 遗传算法](https://www.youtube.com/watch?v=7J-DfS52bnI)
* [obitko.com - 遗传算法教程](https://www.obitko.com/tutorials/genetic-algorithms/index.php)
* [Darrell Whitley - 一种遗传算法教程](https://ibug.doc.ic.ac.uk/media/uploads/documents/courses/GeneticAlgorithm-tutorial.pdf)
