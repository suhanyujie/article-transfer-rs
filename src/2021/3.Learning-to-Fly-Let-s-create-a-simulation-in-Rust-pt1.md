>* Learning to Fly: Let's create a simulation in Rust! (pt 1) 译文（学习飞行：用 Rust 创建一个模拟器 part1）
>* 原文链接：https://pwy.io/en/posts/learning-to-fly-pt1/
>* 原文作者：[Patryk27](https://github.com/Patryk27)
>* 译文来自：https://github.com/suhanyujie/article-transfer-rs/
>* 译者：[suhanyujie](https://github.com/suhanyujie)
>* 译者博客：[suhanyujie](https://ishenghuo.cnblogs.com/)
>* ps：水平有限，翻译不当之处，还请指正。
>* 标签：Rust, simulation,  genetic-algorithm, neural-network, rust, webassembly

系列文章：
* [Learning to Fly: Let's create a simulation in Rust! (pt 1)](https://pwy.io/en/posts/learning-to-fly-pt1/)
* [Learning to Fly: Let's create a simulation in Rust! (pt 2)](https://pwy.io/en/posts/learning-to-fly-pt2/)

In this series we’ll create a simulation of **evolution** using **neural network & genetic algorithm**.
>在这个系列中，我们将使用 **神经网络和遗传算法** 创建一个 **进化** 模拟器。

I’m going to introduce you to how a basic neural network and genetic algorithm works, then we’ll implement both in Rust and compile our application to **WebAssembly** to ultimately end up with:
>我会向你介绍一个基础的神经网络和遗传算法是如何工作的，然后使用 Rust 实现它们，并将应用编译到 **WebAssembly**，最后实现如下所示：

![Figure 1. https://pwy.io/en/projects/shorelark/](https://pwy.io/resources/learning-to-fly-pt1/intro-outcome.png)

>* This series is intended for **Rust beginners** - I’m assuming you know a thing or two about Rust and I’ll introduce you to rest of the concepts (such as neural networks) as we go.
>* 这个系列的文章主要面向 **Rust 初学者** - 我假定你已初步了解 Rust，之后我会向你介绍其他概念（如神经网络）。
>* No fancy mathematics or IT background is required.
>* 不需要熟练掌握花哨的数学和雄厚的 IT 背景。

This series will be divided into a few posts, roughly:
>这个系列将会分为若干篇文章，大概如下：

* 1.Introduction to the domain (what are we going to simulate, how does a neural network & genetic algorithm work),
* 1.领域说明（我们模拟什么，神经网络和遗传算法如何工作），
* 2.Implementing a neural network,
* 2.实现一个神经网络
* 3.Implementing a genetic algorithm,
* 3.实现一个遗传算法
* 4.Implementing eyes, brain, and the simulation itself.
* 4.实现眼睛、大脑和模拟器本身。

Due diligence: I’ll do my best to explain all the concepts, but if at any point you feel lost, feel free to take a look at this article’s last section - it contains links to external (mostly popular science) sources that might prove to be helpful in understanding the domain.
>尽职调查：我会尽我所能解释所有的概念，但如果你在任意时候有疑惑，可以看这篇文章的最后一节 —— 它包含了外部资源（主流科学）的一些链接，可能会帮助你理解对应的领域知识。

Curious? Hop on the bus, Gus, and onto the first chapter: [Design](https://pwy.io/en/posts/learning-to-fly-pt1/#design).
>好奇吗？快上车吧，Gus，我们进入第一章：[设计](https://pwy.io/en/posts/learning-to-fly-pt1/#design).

## [Design](https://pwy.io/en/posts/learning-to-fly-pt1/#design)

Let’s start by clearly defining our objective: what are we going to simulate, actually?
>我们先明确我们的目标：我们到底要模拟什么？

The overall idea is that we have a two-dimensional board representing a **world**:
>总体设想是我们有一个二维的**世界**：

![](https://pwy.io/resources/learning-to-fly-pt1/design-1.png)

This world consists of **birds** (hence the project’s original code name - _Shorelark_):
>这个世界由**鸟**构成（因此项目的仓库名用 —— _Shorelark_）：

![](https://pwy.io/resources/learning-to-fly-pt1/design-2.png)

... and **foods** (of an abstract kind, rich in protein & fiber):
>... 以及**食物**（抽象类实体，富含蛋白质和纤维）：

![](https://pwy.io/resources/learning-to-fly-pt1/design-3.png)

Each bird has their own **vision**, allowing them to locate the food:
>每只鸟有它们的**视界**，它们可以自己寻找食物：

![](https://pwy.io/resources/learning-to-fly-pt1/design-4.png)

... and a **brain** that controls bird’s body (i.e. speed and rotation).
>... 并且由**大脑**控制鸟的身体（比如速度和旋转）。

Our magic touch will lay in the fact that instead of hard-coding our birds to some specific behavior (e.g. "go to the nearest food in your eyesight"), we’ll take a more intriguing route:
>我们的神奇之处在于，我们没有将鸟类硬编码为具有特定的行为的实体（例如：去定位你视线范围内最近的食物），我们会走一条更有趣的路线：

We’ll make our birds able to **learn** and **evolve**.

## Brain

If you squint your eyes hard enough, you’ll see that a brain is nothing but a **function** of some inputs to some outputs, e.g.:

![](https://pwy.io/resources/learning-to-fly-pt1/brain-1.png)

>You’re a precious mathematical formula, remember that.

Since our birds will have only one sensory input, their brains can be then approximated as:

![](https://pwy.io/resources/learning-to-fly-pt1/brain-2.png)

Mathematically, we can represent this function’s input (i.e. biological eye) as a list of numbers, with each number (i.e. biological _photoreceptor_) describing _how close_ the nearest object (i.e. food) is:

![](https://pwy.io/resources/learning-to-fly-pt1/brain-3.png)

_(0.0 - no object in sight, 1.0 - object right in front of us.)_

>Our birds won’t see color, but that’s just for simplicity - you could use e.g. [raytracing](https://raytracing.github.io/books/RayTracingInOneWeekend.html) to make the eyes more realistic.

As for the output, we’ll make our function return a tuple of `(Δspeed, Δrotation)`.

For instance, a brain telling us `(0.1, 45)` will mean "body, please increase our speed by `0.1` units and rotate us `45` degrees clockwise", while `(0.0, 0)` will mean "body, please keep our course steady".

>* It’s important that we use relative values (so `delta speed` and `delta rotation`), as our brain won’t be aware of its own location & rotation respective to the world - passing that information would increase our brain’s complexity for no real benefit.

Finally, let’s address the elephant in the room: so a brain is basically `f(eyes)`, right? But how do we find out what actually follows the equals sign?

```
f(eyes) = what?
```

## Neural network: Introduction

As a fellow human, you are might be aware that brains are made of neurons connected via synapses:

![](https://pwy.io/resources/learning-to-fly-pt1/nn-1.png)

_Figure 2. My attempt at drawing neurons; not to scale_

Synapses carry electric and chemical signals between neurons, while neurons "decide" whether given signal should be propagated further or stopped; eventually this allows for people to recognize letters, eat brussels sprouts, and write mean comments on Twitter.

Due to their [inherent complexity](https://en.wikipedia.org/wiki/Biological_neuron_model), biological neural networks are not among the easiest to simulate (one could argue that neurons are thus not [Web Scale](https://www.youtube.com/watch?v=b2F-DItXtZs)), which made some smart people invent a class of mathematical structures called artificial neural networks, which allow to approximate - with a pinch of salt - brain-like behavior using math.

Artificial neural networks (which I’m going to call just neural networks) are prominent at **generalizing** over datasets (e.g. learning how a cat looks like), so they found their use in face recognition (e.g. for cameras), language translation (e.g. for [GNMT](https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation)), and - in our case - to steer colorful pixels for a handful of reddit karma.

The particular kind of network we’ll be focusing on is called `feedforward neural network` (FFNN)…​

>Cool bear’s hot tip: FFNNs are sometimes called [multilayer perceptrons](https://en.wikipedia.org/wiki/Multilayer_perceptron) and they are one of the building [blocks of convolutional neural networks](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53), such as [DeepDream](https://en.wikipedia.org/wiki/DeepDream).

... and it looks like this:

![](https://pwy.io/resources/learning-to-fly-pt1/nn-2.png)

_Figure 3. Example of a multilayer perceptron (MLP), also called FFNN_

This is a layout of an FFNN with **five synapses** and **three neurons**, all organized in **two layers**: the **input layer** (on the left side) and the **output layer** (on the right side).

There may also exist layers in-between, in which case they are called hidden layers - they improve the network’s ability to understand the input data (think: the bigger the brain, the "more abstraction" it might understand, to a certain degree).

>A [similar process](https://www.youtube.com/watch?v=rA5qnZUXcqo) happens inside your own [visual cortex](https://en.wikipedia.org/wiki/Visual_cortex), too.

Contrary to biological neural networks (which piggyback on electric signals), FFNNs work by accepting some **numbers** at their input and propagating (_feedforwarding_) those numbers layer-by-layer across the entire network; numbers that appear at the last layer determine network’s answer.

For instance, if you fed your network with raw pixels of a picture, you might’ve got a response saying:

* `0.0` - this picture does not contain an orange cat eating lasagna,
* `0.5` - this picture might contain an orange cat eating lasagna,
* `1.0` - this picture certainly contains an orange cat eating lasagna.

It’s also possible for a network to return _many values_ (the number of output values is equal to the number of neurons in the output layer):

* `(0.0, 0.5)` - this picture _does not_ contain an orange cat, but _might_ contain a lasagna,
* `(0.5, 0.0)` - this picture _might_ contain an orange cat, but _does not_ contain a lasagna.

The meaning of input and output numbers is up to you - in this case we’ve simply imagined that there exists some neural network behaving this way, but in reality it’s on you to prepare so-called **training dataset** ("given this picture, you should return 1.0", "given that picture, you should return 0.0").

>You might’ve as well created a network that, say, [identifies mature apples](https://www.researchgate.net/publication/320662740_Identification_and_counting_of_mature_apple_fruit_based_on_BP_feed_forward_neural_network) - sky’s the limit.

Having the general overview of FFNNs in mind, let’s now take the next major step and learn about the magic allowing for all of this to happen.

## Neural network: Deep dive

FFNNs lean on two building blocks: neurons and **synapses**.

A **neuron** (usually represented with a circle) accepts some input values, processes them, and returns some output value; each neuron has at least one input and at most one output:

![](https://pwy.io/resources/learning-to-fly-pt1/nn-3.png)

_Figure 4. A single neuron with three synapses_

Additionally, each neuron has a **bias**:

![](https://pwy.io/resources/learning-to-fly-pt1/nn-4.png)

_Figure 5. A single neuron with three synapses and annotated bias value_

Bias is like a neuron’s `if` statement - it allows for a neuron to stay inactive (return an output of zero) unless the input is strong (high) enough. Formally we’d say that bias allows to regulate neuron’s **activation threshold**.

Imagine you’ve got a neuron with three inputs, with each input determining whether it sees a lasagna (`1.0`) or not (`0.0`) - now, if you wanted to create a neuron that’s activated when it sees at least two lasagnas, you’d simply create a neuron with a bias of `-1.0`; this way your neuron’s "natural" state would be `-1.0` (inactive), with one lasagna - `0.0` (still inactive), and with two - `1.0` (active, voilà).

>If my lasagna metaphor doesn’t appeal to you, you might find [this math-oriented explanation](https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks) more helpful.

Apart from neurons, we’ve got synapses - a **synapse** is like a wire that connects one neuron’s output to another neuron’s input; each synapse is of certain **weight**:

![](https://pwy.io/resources/learning-to-fly-pt1/nn-5.png)

_Figure 6. A single neuron with three synapses with annotated weights_

A weight is a factor (hence the `x` before each number, to emphasize its multiplicative nature), so a weight of:

* `0.0` means that a synapse is effectively dead (it doesn’t pass any information from one neuron into the another),
* `0.3` means that if neuron A returns `0.7`, neuron B will receive `0.7 * 0.3 ~= 0.2`,
* `1.0` means that a synapse is effectively passthrough - if neuron A returns `0.7`, neuron B will receive `0.7 * 1.0 = 0.7`.

Having all this knowledge in mind, let’s go back to our network and fill-in missing weights & biases with some random numbers:

![](https://pwy.io/resources/learning-to-fly-pt1/nn-6.png)

What a beauty, isn’t it?

Let’s see what it thinks of, say, `(0.5, 0.8)`:

![](https://pwy.io/resources/learning-to-fly-pt1/nn-7.png)

To reiterate, we’re interested in the output value of the rightmost neuron (that’s our output layer) - since it depends on two previous neurons (the ones from the input layer), we’re going to start with them.

Let’s focus on the top-left neuron first - to calculate its output, we start by computing a **weighted sum** of all its inputs:

```
0.5 * 0.2 = 0.1
```

... then, we add the bias:

```
0.1 - 0.3 = -0.2
```

... and clamp this value through so-called activation function; activation function limits neuron’s output to a predefined range, simulating the `if`-like behavior.

The simplest activation function is rectified linear unit (`ReLU`), which is basically `f32::max`:

![](https://pwy.io/resources/learning-to-fly-pt1/nn-8.png)

>* Another popular activation function is `tanh` - its graph looks slightly different (like an `s`) and it’s got [different properties](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks).

>* **Activation function affects network’s input and output** - e.g. `tanh` forces a network to work on numbers from range `<-1.0, 1.0>` instead of `<0.0, +inf>`, as compared to `ReLU`.

As you can see, when our weighted-sum-with-a-bias is lower than zero, the neuron’s output will be `0.0` - and that’s exactly what happens to our current output:

```
max(-0.2, 0.0) = 0.0
```

Nice; now let’s do the bottom-left one:

```
# Weighted sum:
0.8 * 1.0 = 0.8

# Bias:
0.8 + 0.0 = 0.8

# Activation function:
max(0.8, 0.0) = 0.8
```

At this point we’ve got the input layer completed:

![](https://pwy.io/resources/learning-to-fly-pt1/nn-9.png)

... which heads us towards the last neuron:

```
# Weighted sum:
(0.0 * 0.6) + (0.8 * 0.5) = 0.4

# Bias:
0.4 + 0.2 = 0.6

# Activation function:
max(0.6, 0.0) = 0.6
```

... and the network’s output itself:

```
0.6 * 1.0 = 0.6
```

Voilà - for the input of `(0.5, 0.8)`, our network responded `0.6`.

_(since it’s just an exercise on a totally made-up network, this number doesn’t mean anything - it’s just some output value.)_

Overall, that’s one of the simplest FFNNs possible - given appropriate weights, it’s able to solve [the XOR problem](https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b), but probably lacks computational capacity to steer a bird.

More complex FFNNs, such as this one:

![](https://pwy.io/resources/learning-to-fly-pt1/nn-10.png)

... work exactly the same way: you just go left-to-right, neuron-by-neuron, computing the outputs, until you squeeze all the numbers from the output layer.

_(on that particular diagram some of the synapses overlap, but it doesn’t mean anything: it’s just that representing multidimensional graphs on a flat screen is unfortunate.)_

At this point you might begin to wonder "wait, how do we know our network’s weights?", and for that I’ve got a simple answer:

**we randomize them! ❤️️**

If you’re accustomed to deterministic algorithms (bubble sort, anyone?), this might feel non-diegetic to you, but it’s the way things go for networks containing more than a few neurons: you cross your fingers, randomize the initial weights, and work with what you got.

Notice I said initial weights - having some non-zero weights in place, there are certain algorithms that you can apply on your network to improve it (so, essentially, to teach it).

One of the most popular "teaching" algorithms for FFNNs is [backpropagation](https://www.youtube.com/watch?v=tIeHLnjs5U8):

You show your network lots (think: hundredths of thousands) of examples in the form of "for this input, you should return that" (think: "for this picture of dakimakura, you should return pillow"), and backpropagation slowly adjusts your network’s weights until it gets the answers right.

>* Or not - a network might get stuck at a [local optimum](https://en.wikipedia.org/wiki/Local_optimum) and "just" stop learning.
>* Also, if you ever find yourself doing a neural network crossword, remember that backpropagation is an example of [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning).

Backpropagation is a great tool if you have a rich set of labeled examples (such as photos or statistics), and that’s why it doesn’t fit our original assumption:

We ain’t no statisticians, the world is a cruel place, so we want for our birds to figure out all the learning on their own (contrary to being given concrete examples of "for this vision, go left", "for this vision, go right").

Solution?

~~biology~~ genetic algorithms and the magic of [large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers)

## Genetic algorithm: Introduction

To recap, from the mathematical point of view our underlying problem is that we have a function ([represented](https://en.wikipedia.org/wiki/Universal_approximation_theorem) using a neural network) that’s defined by a whole lot of **parameters**:

![](https://pwy.io/resources/learning-to-fly-pt1/ga-1.png)

_(I didn’t bother to draw all the weights, but I hope you get the point - there’s a lot of them.)_

Had we represented each parameter with a single-precision floating-point number, a network of mere 3 neurons and 5 synapses could be defined in so many different combinations…​

```
(3.402 * 10^38) ^ (3 + 5) ~= 1.8 * 10^308
```

_[(how-many-floating-point-numbers-are-there)](https://jameshoward.us/2015/09/09/how-many-floating-point-numbers-are-there/)_

... that the universe would sooner meet its [ultimate fate](https://en.wikipedia.org/wiki/Heat_death_of_the_universe) than we’d be done checking them all; we certainly need to be smarter!

>* All the possible sets of parameters are called a **search space**.

Since iterating through our search space looking for the single best answer is off the table, we can focus on a much simpler task of finding a list of _suboptimal_ answers.

And in order to do that, we must **dig deeper**.

## Genetic algorithm: Deep dive

This is a wild carrot together with its domesticated form:

![](https://pwy.io/resources/learning-to-fly-pt1/carrot.jpg)

This domesticated, widely known form didn’t appear out of blue - it’s an outcome of hundredths of years of [selective breeding](https://en.wikipedia.org/wiki/Selective_breeding) with certain factors - like taproot’s texture or color - in mind.

Wouldn’t it be awesome if we could do a similar thing with our neural brains? If we just, like, created a bunch of random birds and selectively bred the ones who seemed the most prominent…​

**hmmm**

As it turns out, we’re not the first to stumble upon this idea - there already exists a widely researched branch of computer science called [evolutionary computation](https://en.wikipedia.org/wiki/Evolutionary_computation) that’s all about solving problems "just the way nature would do".

Out of all the evolutionary algorithms, the concrete subclass we’ll be studying is called [genetic algorithm](https://en.wikipedia.org/wiki/Genetic_algorithm).

>* Similarly as with neural networks, there’s no the genetic algorithm - it’s a variety of different algorithms; so to avoid burning the midnight oil, we’ll take a look at how things work _generally_.

Starting top-bottom, a genetic algorithm starts with a **population**:

![](https://pwy.io/resources/learning-to-fly-pt1/ga-2.png)

A population is built from **individuals** (sometimes called **agents**):

![](https://pwy.io/resources/learning-to-fly-pt1/ga-3.png)

An **individual** (or an **agent**) is a single _possible solution_ to given problem (a population is thus a set of some possible solutions).

In our case, each individual will model a brain (or an entire bird, if you prefer to visualise it this way), but generally it depends on the problem you’re tackling:

* If you were trying to, say, [evolve an antenna](https://en.wikipedia.org/wiki/Evolved_antenna), a single individual would be a single antenna.
* If you were trying to, say, [evolve a query plan](https://www.postgresql.org/docs/8.3/geqo-intro2.html), a single individual would be a single query plan.

>* An individual represents some solution, but not necessarily the best or even a remotely desirable one.

An individual is built from **genes** (collectively named **genome**):

![](https://pwy.io/resources/learning-to-fly-pt1/ga-4.png)

_Figure 7. A genome represented with neural network’s weights; a genome might be a list of numbers, a graph or anything else that is able to model a solution to the problem_

A **gene** is a single parameter that’s being evaluated and tuned by the genetic algorithm.

In our case, each gene will be simply a neural network’s weight, but representing problem’s domain isn’t always this straightforward.

For instance, if you were trying to [help a fellow salesman](https://en.wikipedia.org/wiki/Travelling_salesman_problem), where the underlying problem isn’t based on neural networks at all, a single gene could be a tuple of `(x, y)` coordinates determining a part of a salesman’s journey (consequently, an individual would then describe a salesman’s entire path):

![](https://pwy.io/resources/learning-to-fly-pt1/ga-5.png)

_Figure 8. A hypothetical approach to the travelling salesman problem - each box represents a probable, suggested path for the salesman to travel_

Now, let’s say we’ve got a random population of fifty birds - we pass them to a genetic algorithm, what happens?

Similarly as with selective breeding, genetic algorithm starts by **evaluating** each of the individuals (each of the possible solutions) to see which are the best among the current population.

Biologically, this is an equivalent of taking a stroll to your garden and checking which carrots are the orangest and the yummiest.

Evaluation happens using so-called **fitness function** that returns a **fitness score** quantifying how good a particular individual (so a particular solution) is:

![](https://pwy.io/resources/learning-to-fly-pt1/ga-6.png)

_Figure 9. An example of a fitness function that quantifies carrots by their taproot’s color and radius_

Creating a [usable](https://www.youtube.com/watch?v=7J-DfS52bnI) fitness function is one of the hardest tasks when it comes to genetic algorithms, as usually there are many metrics by which an individual can be measured.

(even our imaginative carrot has at least three metrics: taproot’s color, radius, and taste, that have to be squashed into a single number.)

Fortunately, when it comes to birds, we don’t have much to choose from anyway: we’ll just say that a bird is as good as the amount of food it ate during the course of current **generation**.

A bird who ate `30` foods is better than the one who ate just `20`, simple as that.

>* Negating a fitness function makes a genetic algorithm return the worst solutions instead of the best ones; just an amusing trick to remember for later.

Now, the time has come for the genetic algorithm’s crème de la crème: **reproduction**!

Broadly speaking, reproduction is the process of building a new (hopefully - slightly improved) population starting from the current one.

It’s the mathematical equivalent of choosing the tastiest carrots and planting their seeds.

What happens is that the genetic algorithm chooses two individuals at random (prioritizing the ones with the higher fitness scores) and uses them to produce two new individuals (a so-called **offspring**):

![](https://pwy.io/resources/learning-to-fly-pt1/ga-7.png)

Offspring is produced by taking genomes of both parents and performing [crossover](https://en.wikipedia.org/wiki/Chromosomal_crossover) & [mutation](https://en.wikipedia.org/wiki/Mutation) on them:

![](https://pwy.io/resources/learning-to-fly-pt1/ga-8.png)

>* **Crossover** allows to mix two different gnomes to get an approximate in-between solution, while **mutation** allows to discover new solutions that weren’t present in the initial population.

Both newly-spawned individuals are pushed into the pool of `new population` and the process starts over until the entire new population is built; the current population then gets discarded and the whole simulation starts over on this new (hopefully improved!) population.

As you can see, there’s a lot of **randomness** in the process: we start with a random population, we randomize how the genes are being distributed…​ so…​

this cannot actually work, can it?

## The Code

Let’s end this post with a cliffhanger:

```shell
$ mkdir shorelark
```

Can you guess why I didn’t use `cargo new`?

In the second part we’ll implement a working, bare-bones feed-forward neural network - until then!

## Sources

Here are some of the sources that I’ve personally found useful while learning about topics presented in this article:

**Neural networks:**
* [YouTube, 3Blue1Brown - But what is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk)
* [YouTube, Vsauce - The Stilwell Brain](https://www.youtube.com/watch?v=rA5qnZUXcqo)

**Genetic algorithms:**

* [YouTube, Jeremy Fisher - Genetic Algorithms](https://www.youtube.com/watch?v=7J-DfS52bnI)
* [obitko.com - Genetic Algorithms Tutorial](https://www.obitko.com/tutorials/genetic-algorithms/index.php)
* [Darrell Whitley - A Genetic Algorithm Tutorial](https://ibug.doc.ic.ac.uk/media/uploads/documents/courses/GeneticAlgorithm-tutorial.pdf)
