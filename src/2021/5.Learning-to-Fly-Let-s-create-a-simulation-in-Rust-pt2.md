>* Learning to Fly: Let's simulate evolution in Rust! (pt 2) 译文（学习飞行：用 Rust 模拟种群进化 part2）
>* 原文链接：https://pwy.io/en/posts/learning-to-fly-pt2/
>* 原文作者：[Patryk27](https://github.com/Patryk27)
>* 译文来自：https://github.com/suhanyujie/article-transfer-rs/
>* 译者：[suhanyujie](https://github.com/suhanyujie)
>* 译者博客：[suhanyujie](https://ishenghuo.cnblogs.com/)
>* ps：水平有限，翻译不当之处，还请指正。
>* 标签：Rust, simulation,  genetic-algorithm, neural-network, rust, webassembly

系列文章：
* [The Domain (pt 1)](https://pwy.io/en/posts/learning-to-fly-pt1/)
* [The Neural Network (pt 2)](https://pwy.io/en/posts/learning-to-fly-pt2/)
* [The Genetic Algorithm (pt 3)](https://pwy.io/en/posts/learning-to-fly-pt3/)

This is second part of the Learning to Fly series in which we’re coding a simulation of evolution using neural network and genetic algorithm:

![](https://pwy.io/resources/learning-to-fly-pt1/intro-outcome.png)

Figure 1. https://pwy.io/en/projects/shorelark/

In this post we’ll lay the foundations for our project and implement a basic feed-forward neural network that’ll later become a brain; we’ll also take a look at many intricacies and idioms you might find in common Rust code, including those inside tests.

Strap your straps, seatbelt your seatbelts, and get ready for some coding!

## Workspaces

Our previous post ended with a cliffhanger:

Let’s end this post with a cliffhanger:

```
$ mkdir shorelark
```

Instead of launching cargo new shorelark, as one usually does, we went with mkdir - it’s because we’re going to use a feature of Cargo’s called workspaces.

Workspaces allow to split a single project into multiple, standalone crates (separate "subprojects"), which has many advantages:

* it makes it easier for humans to reason about project’s structure (as each crate is more or less self-contained),
* it’s easy to setup & extend (crates can be kept in the same repository or scattered across different ones, whatever’s more convenient for you),
* it allows for Cargo to compile project’s code in parallel.

Let’s focus on the last point.

You might’ve noticed that when you do cargo build, it sometimes prints crate names inline during the Building phase:

```
Updating crates.io index
 Downloaded log v0.4.13
 Downloaded serde v1.0.119
 Downloaded thread_local v1.1.0
 Downloaded byteorder v1.4.2
  Compiling libc v0.2.82
  Compiling cfg-if v1.0.0
  Compiling arrayref v0.3.6
  Compiling matches v0.1.8
   Building [>           ] 6/153: byteorder, matches, cc, byte-tools
```

It means that it’s building those crates in parallel.

Usually only some parts of the dependency tree can be built in parallel - if you have crate A that depends on crates B + C, the latter two can be built at the same time, but A has to wait until B and C have both finished compiling.

Furthermore, it’s pointless trying to simultaneously compile more crates than the amount of CPU cores available, as it would only slow down the process, so Cargo has a tall order figuring it out.

On the other side of the spectrum, we’ve got rustc - it’s the actual compiler that Cargo invokes for each crate it has to built, and rustc itself is at the moment mostly single-threaded.

It means that, practically, if your application depends on some external crates (e.g. from crates.io), those crates will be compiled in parallel, but your application itself (as a single-crate entity) will remain built on a single core.

To internalize the potential issues & gains, let’s say we’re working on a 100k LOC application - if compiling our app, excluding its dependencies, takes 5 minutes now, then by splitting it into 5 separate 20k LOC crates, we might reduce this time to as low as 1~2 minutes.

(due diligence: that’s a good-day scenario, sometimes it might not be possible to split the code, please remember to always consult your doctor before refactoring.)

Granted - in the grand scheme of things, 3 minutes might not seem like much, but let’s not forget that it applies to the development process, too.

If you’ve already had the pleasure of working on a gigantic project, you might know the pain of waiting for the compiler to finish: you’ve changed some error message, wanted to quickly see how it looks in the context and - ｏｈ ｇｏｄ - do you hear this sound? it’s not a military aircraft, it’s your computer’s fans preparing for some hot & long code munching; it’s daunting.

When it comes to Rust, one of the ways to alleviate issues around compile times is then to simply split your application into separate crates.

One of the bigger - if not the biggest - workspace-based project I know is the almighty rustc.

Considering our toy project won’t reach 10k LOC, why am I even talking about all those workspaces and crates?

I just think they’re neat.

Also, I consider them to be a good practice: workspaces allow to introduce clear-cut boundaries between project’s modules, and - when applied within reason - not only make the code cleaner, but force it to be.

Having established that workspaces are not elder magic, let’s circle back to our original question: why mkdir instead of cargo new?

Simply because Cargo doesn’t support cargo new --workspace yet.

## Structure

Breath in, breath out, and let’s enter the no man’s land:

```
$ cd shorelark
$ ls -al
# total 0
# drwxr-xr-x  2 ppp users  40 01-16 19:14 .
# drwxrwxrwt 22 ppp users 520 01-16 20:27 ..
```

Before we’re able to write our first line of code, we have to decide on our project’s structure.

There are many approaches for organizing workspace-based projects - I’m personally fond of this one, which separates application-like crates from those library-like ones:

```
project
├─ Cargo.toml
├─ app
│  ├─ Cargo.toml
│  └─ src
│      └─ main.rs
└─ libs
   ├─ subcrate-a
   │  ├─ Cargo.toml
   │  └─ src
   │     └─ lib.rs
   └─ subcrate-b
      ├─ Cargo.toml
      └─ src
         └─ lib.rs
```

Another popular approach is to simply dump all crates into the same directory called e.g. src or crates.

Scaffolding such project is as easy as creating a Cargo.toml manifest with:

```
[workspace]
members = [
    "libs/*", # look má, wildcards!
]
```

Most manifests contain a section called [package] that defines crate’s metadata:

```
[package]
name = "brexit-loss-calculator"
version = "0.0.0"
authors = ["B.J."]
edition = "2018"
```

Our Cargo.toml does not, as it’s used to merely group all the packages together without being a crate on its own; formally speaking, our Cargo.toml is a virtual manifest.

... and then proceeding with cargo new --lib as usual:

```
$ mkdir libs
$ cd libs
$ cargo new neural-network --lib
```

## Coding: propagate()

It’s time to get down to business.

We’ll start top-down, with a structure modelling the entire network - it will provide sort of an entry point to our crate; let’s open libs/neural-network/src/lib.rs and write:

```rust
pub struct Network;
```

A neural network’s most crucial operation is propagating numbers:

![](https://pwy.io/resources/learning-to-fly-pt2/coding-propagate-1.svg)

... so:

```rust
impl Network {
    pub fn propagate(&self, inputs: Vec<f32>) -> Vec<f32> {
        todo!()
    }
}
```

While some languages allow to leave "work-in-progress" functions empty:

```
int get_berry_number() {
    // TODO solve the paradox
}
```

... an equivalent code in Rust does not compile:

```rust
fn berry_number() -> usize {
    // TODO solve the paradox
}
```

```
error[E0308]: mismatched types
 --> src/lib.rs
  |
1 | fn berry_number() -> usize {
  |    ------------      ^^^^^ expected `usize`, found `()`
  |    |
  |    implicitly returns `()` as its body has no tail or `return`
  |    expression
```

It’s due to the fact that almost everything in Rust is an expression:

```rust
let value = if condition {
    "computer says yass"
} else {
    "computer says no"
};

let value = loop {
    break 123;
};

let value = {
    // empty block is an expression, too
};
... and so the way Rust sees that function is actually:

fn berry_number() -> usize {
    return ();
}
```

... with () being called unit value (or unit type, depending on the context).

To answer the problem of but i really don’t know how this function should look like just yet, Rust provides two macros: todo!(), and its older cousin - unimplemented!().

Both macros allow for the code to be compiled and, when encountered during runtime, cause the application to safely crash:

```
thread 'main' panicked at 'not yet implemented'
```

Similarly to an ocean filled with water droplets, a network is built from layers:

![](https://pwy.io/resources/learning-to-fly-pt2/coding-propagate-2.svg)

... so:

```rust
pub struct Network {
    layers: Vec<Layer>,
}

struct Layer;
```

Layers are built from neurons:

![](https://pwy.io/resources/learning-to-fly-pt2/coding-propagate-3.svg)

... giving us:

```rust
struct Layer {
    neurons: Vec<Neuron>,
}
```

Eventually, neurons contain biases and output weights:

![](https://pwy.io/resources/learning-to-fly-pt2/coding-propagate-4.svg)

... so:

```rust
struct Neuron {
    bias: f32,
    weights: Vec<f32>,
}
```

Let’s see our crude design in its entriety:

```rust
pub struct Network {
    layers: Vec<Layer>,
}

struct Layer {
    neurons: Vec<Neuron>,
}

struct Neuron {
    bias: f32,
    weights: Vec<f32>,
}

impl Network {
    pub fn propagate(&self, inputs: Vec<f32>) -> Vec<f32> {
        todo!()
    }
}
```

Nice.

If you’re lucky or perceptive, you might’ve noticed that only two of our objects are public: Network and Network::propagate() - it’s because Layer and Neuron will remain an implementation detail, we won’t expose them outside.

Thanks to this approach, we’ll be able to introduce changes to our implementation without imposing breaking changes on the downstream crates (i.e. our library’s users).

For instance, professional neural networks (for performance reasons) are usually implemented using matrices - if we ever decided to rewrite our network to use matrices, then it wouldn’t be a breaking change: Network::propagate()-s signature would remain the same and since users can’t access Layer and Neuron, they wouldn’t be able to notice these two structs being gone.

Going next - since numbers have to be shoved through each layer, we’ll need to have a propagate() in there, too:

```rust
impl Layer {
    fn propagate(&self, inputs: Vec<f32>) -> Vec<f32> {
        todo!()
    }
}
```

Having Layer::propagate(), we can actually go back and implement Network::propagate():

```rust
impl Network {
    fn propagate(&self, inputs: Vec<f32>) -> Vec<f32> {
        let mut inputs = inputs;

        for layer in &self.layers {
            inputs = layer.propagate(inputs);
        }

        inputs
    }
}
```

This is quite a satisfying, correct piece of code - but it’s also non-idiomatic: we can write it better, more rustic; let’s see how!

![](https://pwy.io/resources/learning-to-fly-pt2/coding-propagate-5.svg)

First of all, this is called a rebinding (or shadowing):

```rust
let mut inputs = inputs;
```

... and it’s unnecessary, because we might as well move this mut into function’s parameters:

```rust
fn propagate(&self, mut inputs: Vec<f32>) -> Vec<f32> {
    for layer in &self.layers {
        inputs = layer.propagate(inputs);
    }

    inputs
}
```

But hey, won’t this force our callers to pass mutable values? Nope!

```rust
fn process(mut items: Vec<f32>) {
    // do something
}

fn main() {
    let items = vec![1.2, 3.4, 5.6];
    // ^ no `mut` needed here

    process(items);
    //      ^ just works
}
```

[(playground link)](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=945c2c2852ccbeb143ea569747038626)

The reasoning is that the mut we’ve just introduced appears in so-called binding position:

```rust
fn foo_1(items: &[f32]) {
    //   ^^^^^  ------
    //  binding  type
    // (immut.) (immut.)
}

fn foo_2(mut items: &[f32]) {
    //   ^^^^^^^^^  ------
    //    binding    type
    //   (mutable) (immut.)
}

fn foo_3(items: &mut [f32]) {
    //   ^^^^^  ----------
    //  binding    type
    // (immut.)  (mutable)
}

fn foo_4(mut items: &mut [f32]) {
    //   ^^^^^^^^^  ----------
    //    binding      type
    //   (mutable)   (mutable)
}

struct Person {
    name: String,
    eyeball_radius: usize,
}

fn decompose(Person { name, mut eyeball_radius }: Person) {
    //       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  ------
    //                     binding                 type
    // (partially immutable, partially mutable) (immutable)
}
```

... and bindings, contrary to types, are local to a function:

```rust
fn foo(items: &mut Vec<usize>) {
    // When a type is mutable, you can modify the thing being
    // referenced:
    items.push(1234);

    // But if the binding remains immutable, you cannot modify
    // *which* thing is referenced:
    items = some_another_vector;
    //    ^ error: cannot assign to immutable argument
}

fn bar(mut items: &Vec<usize>) {
    // On the other hand, when a binding is mutable, you can change
    // *which* thing is referenced:
    items = some_another_vector;

    // But if the type remains immutable, you cannot modify the
    // thing itself:
    items.push(1234);
    //   ^^^^^ error: cannot borrow `*items` as mutable, as it is
    //         behind a `&` reference
}
```

There’s one more refinement we can apply to our code - this very pattern is known as folding:

```rust
for layer in &self.layers {
    inputs = layer.propagate(inputs);
}
```

... and Rust’s standard library provides a dedicated function for it:

```rust
fn propagate(&self, inputs: Vec<f32>) -> Vec<f32> {
    self.layers
        .iter()
        .fold(inputs, |inputs, layer| layer.propagate(inputs))
}
```

(one could argue whether our final code is actually more readable or less; while I’m personally fond of the built-in combinators such as .fold(), if you find them obscure - that’s fine, too! you do you, i ain’t no judge)

Voilà - after all, thanks to the closure, we didn’t even need that mut inputs; now you can brag about your code being all functional and Haskell-y.

Let’s move on to neurons - a single neuron accepts many inputs and returns a single output, so:

```rust
struct Neuron {
    bias: f32,
    weights: Vec<f32>,
}

impl Neuron {
    fn propagate(&self, inputs: Vec<f32>) -> f32 {
        todo!()
    }
}
```

As before, we can backtrack to implement Layer::propagate():

```rust
struct Layer {
    neurons: Vec<Neuron>,
}

impl Layer {
    fn propagate(&self, inputs: Vec<f32>) -> Vec<f32> {
        let mut outputs = Vec::new();

        for neuron in &self.neurons {
            let output = neuron.propagate(inputs);
            outputs.push(output);
        }

        outputs
    }
}
```

If we try to compile it, we get our first borrow-checker failure:

```
error[E0382]: use of moved value: `inputs`
  --> src/lib.rs
   |
   |     fn propagate(&self, inputs: Vec<f32>) -> Vec<f32> {
   |                         ------ move occurs because `inputs` has
   |                                type `Vec<f32>`, which does not
   |                                implement the `Copy` trait
...
   |             let output = neuron.propagate(inputs);
   |                                           ^^^^^^
   |                                value moved here, in previous
   |                                iteration of loop
```

Obviously, the compiler is right: after invoking neuron.propagate(inputs), we lose ownership of inputs, so we can’t possibly use it inside loop’s consecutive iterations.

Fortunately, the fix is rather easy and boils down to making Neuron::propagate() work on borrowed values:

```rust
impl Layer {
    fn propagate(&self, inputs: Vec<f32>) -> Vec<f32> {
        /* ... */

        for neuron in &self.neurons {
            let output = neuron.propagate(&inputs);
            //                            ^
        }

        /* ... */
    }
}

impl Neuron {
    fn propagate(&self, inputs: &[f32]) -> f32 {
        //                      ^^^^^^
        /* ... */
    }
}
```

To reiterate, the code we have at the moment is:

```rust
impl Layer {
    fn propagate(&self, inputs: Vec<f32>) -> Vec<f32> {
        let mut outputs = Vec::new();

        for neuron in &self.neurons {
            let output = neuron.propagate(&inputs);
            outputs.push(output);
        }

        outputs
    }
}
```

The way we wrote our algorithm is correct, but inefficient - since we know how many output values we’ll have, we can use this information to preallocate our vector:

```rust
fn propagate(&self, inputs: Vec<f32>) -> Vec<f32> {
    let mut outputs = Vec::with_capacity(self.neurons.len());

    /* ... */
}
```

In order to understand preallocation, we’ve gotta go one abstraction layer below and take a look at how Vec works.

Since Vec doesn’t know how many elements it will contain, it starts empty and then every other invocation of .push() causes it to grow larger and larger. Growing is a "relatively slow" operation, because it requires for the vector’s entire memory to be moved into another place in RAM that contains enough space.

If we know - or we can estimate - a vector’s size up-front, we might then create it using Vec::with_capacity(), which accepts a single argument determining how big the returned vector should be; e.g. Vec::with_capacity(10) means that you’ll be able to use .push() at least 10 times without causing the returned vector to grow.

>Vec::with_capacity() can be a great low-hanging fruit if your application allocates lots of large vectors, but using it is not always worth the hassle; as usually when it comes to performance, a benchmark’s your best friend.

Using with_capacity() might be awkward at times and so Rust, fulfilling its promise of zero-cost abstractions, allows to invoke it somewhat transparently:

```rust
fn propagate(&self, inputs: Vec<f32>) -> Vec<f32> {
    self.neurons
        .iter()
        .map(|neuron| neuron.propagate(&inputs))
        .collect()
}
```

There’s no explicit Vec::with_capacity() and yet both codes do essentially the same (including preallocation!) - how come?

Well, iterators contain a method called Iterator::size_hint() - it returns an iterator’s length (or None, if the length’s not known). When you do vec.iter(), it simply creates an iterator that knows its length and then .collect() uses that information to automatically create a vector that’s large enough; no magic!

As you can see, even though our refactored code is technically more complex (we’re using an anonymous function and iterators, after all), it’s at least equally performant and way more readable; writing optimized Rust code frequently cuts down to using high-level structures instead of trying to outsmart the compiler with fancy, hand-written loops.

Currently we’ve got nowhere else to go, but to complete Neuron::propagate() - as before, let’s start with a crude, superfund-ish version:

```rust
impl Neuron {
    fn propagate(&self, inputs: &[f32]) -> f32 {
        let mut output = 0.0;

        for i in 0..inputs.len() {
            output += inputs[i] * self.weights[i];
        }

        output += self.bias;

        if output > 0.0 {
            output
        } else {
            0.0
        }
    }
}
```

This snippet contains two unidiomatic constructs and one potential bug - let’s start with the latter.

Since we’re iterating through self.weights using length of `inputs’, there are three edge cases we have to consider:
* 1.Given inputs.len() < self.weights.len(),
* 2.Given inputs.len() == self.weights.len(),
* 3.Given inputs.len() > self.weights.len().

Our code lays on the assumption that #2 is always true, but it’s a silent assumption: we don’t enforce it anywhere! If we mistakenly passed less or more inputs, we’d get either an invalid result or a crash.

There are at least two ways we could go around improving it:

* 1.We could change Neuron::propagate() to return an error message:

```rust
fn propagate(&self, inputs: &[f32]) -> Result<f32, String> {
    if inputs.len() != self.weights.len() {
        return Err(format!(
            "got {} inputs, but {} inputs were expected",
            inputs.len(),
            self.weights.len(),
        ));
    }

    /* ... */
}
```

... or, using one of the crates I love the most - thiserror:

```rust
pub type Result<T> = std::result::Result<T, Error>;

#[derive(Debug, Error)]
pub enum Error {
    #[error(
        "got {got} inputs, but {expected} inputs were expected"
    )]
    MismatchedInputSize {
        got: usize,
        expected: usize,
    },
}

/* ... */

fn propagate(&self, inputs: &[f32]) -> Result<f32> {
    if inputs.len() != self.weights.len() {
        return Err(Error::MismatchedInputSize {
            got: inputs.len(),
            expected: self.weights.len(),
        });
    }

    /* ... */
}
```

* 2.We could use the assert_eq!() / panic!() macros:

```rust
fn propagate(&self, inputs: &[f32]) -> f32 {
    assert_eq!(inputs.len(), self.weights.len());

    /* ... */
}
```

In most cases, the first variant is better, because it allows for the caller to catch the error and handle it - in our case though, the error reporting is simply not worth it, because:

* 1.If this assertion ever fails, it means that our implementation is most likely wrong and there’s nothing users could do on their side to mitigate it.
* 2.This is a toy project, we’ve already got like fifty other ideas hanging in the air tonight, no need to waste our time.

So:

```rust
fn propagate(&self, inputs: &[f32]) -> f32 {
    assert_eq!(inputs.len(), self.weights.len());

    let mut output = 0.0;

    for i in 0..inputs.len() {
        output += inputs[i] * self.weights[i];
    }

    output += self.bias;

    if output > 0.0 {
        output
    } else {
        0.0
    }
}
```

As for the idioms - this one:

```rust
if output > 0.0 {
    output
} else {
    0.0
}
```

... is f32::max() in disguise:

```
output.max(0.0)
```

While this one:

```rust
let mut output = 0.0;

for i in 0..inputs.len() {
    output += inputs[i] * self.weights[i];
}
```

... can be simplified first with .zip():

```rust
let mut output = 0.0;

for (&input, &weight) in inputs.iter().zip(&self.weights) {
    output += input * weight;
}
```



